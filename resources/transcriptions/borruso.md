Allora, quanti di voi hanno più di cinque amici diventati esperti di intelligenza artificiale? 10? Io penso pure io. Io non lo sono. Oggi vi racconto un'esperienza da utente e perché, dal punto di vista di chi utilizza la riga di comando per stare su operazioni di lettura e trasformazione dati, da utente, ti cambia le carte in tavola.

Io mostrerò uno strumento in particolare perché, secondo me, è lo strumento che più sembra uno strumento di base, come nella shell sono `awk`, `less`. È uno strumentino molto orizzontale con cui ci si interfaccia a qualsiasi, eh, modello: modello locale, modello via API. È molto facile da utilizzare, chiaramente, se si sta dentro questi contesti. E inizio, sennò bla bla bla.

Allora, iniziamo da una cosa buffa ma molto vera. Eh, non è mia, è una bellissima vignetta di xkcd, eh, in cui due amici si parlano. C'hanno la bomba atomica che sta per scoppiare (in questi giochi tra gli argomenti fa un po' schifo, ma comunque...). Solo che, per bloccare il conto alla rovescia, devi saper utilizzare il mitico comando `tar`, e ogni volta devi andare a leggere il manuale, ricordarti, cercare nella history... e difatti la vignetta finisce con la roba che, insomma, è stata scoppiata. Perché, comunque, ricordarsi come funziona `tar`, obiettivamente è impossibile.

C'è poco da ridere, nel senso che, eh, facciamo un salto indietro di qualche anno, quando non c'era l'intelligenza artificiale. Non lo so dire, non so perché. Cercavamo su Stack Exchange, cercavamo su Google, su Reddit, chiedevamo all'amico &quot;psicotico&quot;, quello bravo che sapeva fare. &quot;No, io a questo non ci arrivo&quot;. &quot;Non è cosa, psicotico!&quot;. &quot;Sì, ma non sapevo questa cosa&quot;. &quot;Ci arrivi, dai!&quot;. Esatto. Ragazzi, quella è un'altra vignetta meravigliosa. Oppure, utilizzare `man`. Apriamo la riga di comando. Se io faccio `man tar`, quindi il manuale di `tar`, eh, mi leggo pagine e pagine, devo andare a capire il mio caso specifico. Vabbè, diciamo, se divento un utente esperto, ci mancherebbe, è un comando molto facile. Per un utente una tantum, la vignetta funziona molto bene.

Ehm, ma ora c'è l'intelligenza artificiale e non scoppiano più le bombe. Vabbè, diciamo, in realtà si pigliano sempre cantonate se si usa male, soprattutto. Eh, ma a quanti token corrisponde questo manuale? Il token è importante conoscerlo, no? È l'unità di misura dello scambio dati tra noi e i motori di intelligenza artificiale. Il token lo paghiamo. Ci sono modelli che hanno un *free tier* e, quindi, un certo numero di token gratuiti in ingresso e in uscita, e iniziare a prendere le misure di queste cose diventa essenziale quando poi si vuole andare sia a testare, sia ad andare in produzione. Diciamo che i token, più o meno, sono tre-quattro lettere, una parola... dipende un po' dal modello di tokenizzazione. Ma, per esempio, con questo strumentino a riga di comando che si chiama `tiktoken`, è molto comodo perché passi direttamente in *pipe* qualsiasi contenuto testuale: un XML, un Markdown, un CSV, un JSON. `pipe tiktoken` e hai il numero di token.

L'altra cosa carina che puoi fare, eh, gli puoi dire: &quot;Dammi i primi 1000 token&quot;. Questo ti può servire perché, se sai che stai usando un modellino che ha dei limiti in ingresso, devi capire se il tuo ragionamento è compatibile con quel numero di token. Simon Willison è, per me, la persona di riferimento più importante nel mondo dell'intelligenza artificiale. È l'autore di `tiktoken`, ma è l'autore dell'utility che vedremo tra poco, che si chiama LLM. Sembra un nome di fantasia ma, diciamo, è un nome molto, molto appropriato. E se non lo conoscete, iscrivetevi alla sua newsletter e al suo blog: è straordinario.

Lavorando alla vecchia maniera... scusate, è il fatto di essere l'ultimo e che è tardi. Allora, chiaramente, alla vecchia maniera, se io mi misuro con il buon `wc` del manuale, quindi &quot;contami le linee&quot;, sono 956 linee di manuale. E — vado alle slide, non faccio tutti i comandi — sono 5.190 parole e 9.008 token. Insomma, poca roba, nel senso che la gran parte dei modelli e delle API che utilizziamo di più stanno di base su 128.000 token in ingresso. Ci sono adesso *free tier* con 1 milione di token, con una chiamata gratis ogni, credo, 4-5 secondi, e poi per utilizzi intensi diventano pochi anche quelli.

E a quel punto utilizzo l'utility che si chiama LLM. L'utility di Python si installa con `pip` o con `uv`, e gli passo i dati tramite *standard input*/*standard output* — una cosa vecchia quasi 50 anni, insomma, parliamo dei primi computer Unix, Linux. Vi faccio la domanda, il classico *prompting*, come si usa. Prendo come input tutto il manuale di `tar`, perché io non so niente, lo passo a LLM e gli chiedo in linguaggio naturale: &quot;Crea un archivio `tar` dal file `appunti.tar.gz`. Scrivi soltanto il comando e commenta le opzioni&quot;. Se non gli scrivo questo, mi vomita il papello della storia di `tar`. Quindi il *prompt engineering*, scrivere bene le domande, ovviamente è fondamentale, è molto più importante degli strumenti.

Eh, qual è la risposta? Mi dice: &quot;Guarda, devi utilizzare `tar -xzf`&quot;. Me lo dimentico sempre. È carino perché, poi, mi fa l'elenco puntato in Markdown, quindi può diventare una guida HTML in cui mi dice che il parametro `-x` fa questo, `-z` fa quello... e quindi rimango nel mio contesto operativo, che è la shell, e rilancio il comando subito dopo. E, appunto, sempre *standard output* e *standard in*, quindi tutto chiavi in mano, funzionale.

L'utility si chiama LLM, non come il concetto di *large language model* (ovviamente cita quello), e l'autore è Simon Willison. La cosa bella è che lo potete utilizzare come libreria Python, quindi se vi piace come funziona, se vi sembra veramente *easy*, lo potete mettere in *pipeline* di codice Python più robuste e complesse, con più verifiche di errore rispetto a un classico script Bash. È anche il creatore di Datasette, che per noi che ci occupiamo di dati è un'applicazione straordinaria per pubblicare dati ed API su interfacce web.

Esistono decine di plugin per LLM. Tendenzialmente, la gran parte serve ad aggiungere modelli o interazioni con API. Da pochissimo hanno rilasciato le API per parlare con i modelli disponibili in GitHub Copilot, che sono decine. Anche qui avete il *free tier* o, se siete abbonati, avete i token del vostro abbonamento. I modelli si elencano col comando `models`, e questi che vedete sono quelli installati sulla mia macchina: Anthropic, Gemini, OpenAI, Ollama. Potete anche lavorare con modelli in locale se avete una macchina che ve lo consente.

Il modello di default è GPT-4o, ma basta che vi ricavate delle chiavi API e potete usare quello che volete. La cosa comoda è che, potendo usare 50-60 modelli, posso fare dei test per capire qual è il migliore per certe cose. Chiaramente, è inutile fare a tentativi: bisogna studiare cosa fanno i modelli rispetto al nostro obiettivo. Questo è un esempio didattico: faccio un semplice loop su tre importanti LLM generalisti (Claude Sonnet, Gemini 1.5 Flash e ChatGPT-4o) e gli dico: &quot;Dammi un nome per un convegno su...&quot;. Lo mando in loop e ottengo tre risultati. Niente di emozionante, ma pensate ai casi operativi veri.

La cosa interessante è che potete impostare i parametri, come la temperatura. Su ogni modello, potete guardarvi le opzioni. Qui stiamo guardando Gemini 1.5 Flash: espone la temperatura (quanto è fantasioso), il numero massimo di token in output, se abilitare Google Search, se usare file allegati (GIF, PDF, MP3). Ogni modello ha le sue specifiche. Per curiosità, vi faccio vedere un altro modello, quello di OpenAI: anche qui abbiamo la temperatura, che ti dice può variare tra 0 e 2. Nelle opzioni di Gemini non c'è scritto, bisogna guardare la documentazione. Grande facilità: ho fatto un loop in 2 secondi, ho parlato con tre modelli e posso usare parametri seri per bilanciare le mie richieste.

Ma torniamo alla shell. Il comando `lsb_release -a` ti stampa il sistema operativo. Io ho installato Debian dentro WSL 2. Lui mi dice: `Description: Penguin`, `Release: 12`. Chi fa questo mestiere non ha bisogno di spiegazioni, ma è interessante che, essendo nella shell, è tutto testo e posso usare comandi da *subshell*. Quindi, lancio un *prompt*: &quot;Quali sono i punti chiave del mio OS?&quot;. Il comando viene eseguito, l'output passato a LLM che mi risponde: &quot;I punti chiave sono: è una distribuzione Debian, versione 12, nome in codice Bookworm...&quot;. Di default, l'output è in Markdown, quindi facilmente trasformabile in HTML.

Il web è testo, una delle principali fonti per fare cose. Vi faccio vedere questo esempio. &quot;Sì, eseguila! Faccela vedere!&quot;. Ve la faccio vedere. Allora, sto chiedendo sull'homepage de *Il Post* quali sono le categorie di notizie più presenti: Israele ed Iran, Elly Schlein, cronaca, ecc. Un comandino `curl` manda tutto il contenuto, anche CSS e JavaScript, sprecando un po' di token. Lui è un LLM e mi risponde in linguaggio naturale.

Potrei fare una specie di *scraping* ma parlandogli. Voglio analizzare solo la sezione &quot;Storie e Idee&quot;. E allora, dialogo. Gli dico: &quot;Estrai soltanto i titoli della sezione Storie e Idee&quot;. E lui li estrae: &quot;I nostri insegnanti si ricordano di noi&quot;, &quot;Dieci figli all'estero&quot;, ecc. Non sto facendo XPath o selettori CSS. Potrei anche fare una ricerca semantica, per esempio sulla parola chiave &quot;Palestina&quot;, e lui troverebbe titoli correlati, mappando anche &quot;Israele&quot;.

Io però voglio testo strutturato, e allora aggiungo l'opzione `--schema-multi` e chiedo &quot;titolo&quot; e &quot;URL&quot;. Devo fare un bot che ogni giorno estrae questi dati per un'analisi. Ed è eccezionale... nel senso che... zero. Questo è il bello della diretta. Se avesse funzionato, avreste visto titolo e URL. Fidatevi, vi faccio altri esempi.

Il testo strutturato lo posso generare anche per creare dati *fake*. (&quot;Ci segnalano che passi davanti alla webcam&quot;. &quot;Ma digli che è gratis!&quot;. Vabbè, sto fermo). Adesso genero dati *fake*. Con `--schema-multi` do io uno schema fittizio: nome, città, indirizzo, posti a sedere (come intero, `int`). Sto creando cinque ristoranti italiani fittizi e ottengo il JSON. Nel JSON, `posti_a_sedere` è un numero intero, senza virgolette. È comodissimo poter generare dati *fake*.

Guardiamo quest'altro, ancora più carino. Voglio ricavare i nomi dei consiglieri comunali d'Italia. C'è da spararsi, perché i siti sono tutti diversi. Siamo a Palermo. Obiettivo: estrarre nome, cognome, URL della pagina del consigliere e il suo ruolo. Ho scritto un comando in cui passo con `curl` l'URL del sito e dettaglio lo schema: `nome: il nome del consigliere`, `ruolo: il ruolo`, `url: l'URL della pagina`. E poi un dato calcolato: il sesso. In pagina non c'è, ma lui, leggendo &quot;Mario&quot;, metterà &quot;m&quot;. Ho previsto &quot;m&quot;, &quot;f&quot; e &quot;na&quot; (per dubbio/inapplicabile). Il *prompt* è: &quot;Estrai i dati dei consiglieri comunali presenti nella pagina web&quot;. Sta funzionando. Molto interessante: ha fatto *scraping* e ha compreso da solo nomi e ruoli, trovando persino una &quot;consigliere comunale dimissionaria&quot;.

(&quot;Domanda da casa: visto che c'è la foto, potrebbe stabilire il genere analizzandola?&quot;). Ottima domanda. Sì, si può fare e funziona. Possiamo passargli la foto come allegato o lo *screenshot* della pagina. (&quot;L'output in JSON non glielo chiedi tu?&quot;). No, è nativo, è il formato più comodo e quello che le API degli LLM usano. Trasformarlo in CSV è facile. I dati sono lì. Questo è un JSON piatto che posso esplorare con strumenti come `jq`. Sono a riga di comando, posso trasformare e leggere subito.

(&quot;Per fare questa estrazione devo usare per forza un token a pagamento?&quot;). No, il *free tier* di Gemini va benissimo. La cosa carina è che quando scrivo `nome`, `ruolo`, `url`, sto definendo uno schema. Ogni schema ha un ID e una mappatura. La mappatura *full* è tipo JSON API, parlante. Questo schema lo posso usare per controlli e rettifiche. È *human-readable* e *machine-readable*.

La cosa bella? Ho creato questo schema, e ora lo uso per un altro comune. Andiamo a Enna. Il sito è simile, ma non uguale. Utilizzo lo stesso schema, passandogli l'ID. Fantastico! Chiaramente, funzionerà su 10 siti su 8.000, ma con una buona progettazione su *task* verticali, potete riutilizzare gli schemi.

Esempio dell'immagine, per Totò. Passo il *template* &quot;consiglio comunale&quot; e con `--attachment` gli do una PNG. L'input è un'immagine con sei consiglieri. Lo schema è lo stesso, ma il lavoro è diverso: è come se facesse OCR. Otteniamo lo stesso schema, ma senza l'URL, che non si può recuperare da un'immagine.

Ora descriviamo una tabella. Creiamo una tabellina *fake* di cinque persone, la trasformiamo in CSV. Anno, mese, giorno, età come numero intero. Mi sono generato il dato fittizio. L'obiettivo è descriverlo. Gli chiedo: &quot;Descrivimi in modalità Frictionless il file `persone.csv`, usando queste specifiche&quot;. Le specifiche gliele passo io. `cat frictionless.json`. Questo è il JSON con le specifiche Frictionless, uno standard internazionale per descrivere dati, citato anche nelle linee guida *open data*. Lui fa *reverse engineering*: guarda il CSV e lo descrive secondo lo standard. Il comando ha di speciale `-f frictionless.json`. È come fare un RAG: &quot;Per rispondere, tieni conto di questo testo&quot;.

(&quot;Scusa, e se due persone hanno lo stesso cognome? Ho visto che ha messo `primaryKey: cognome`&quot;). È un'allucinazione. Lui ha pensato: &quot;Sarebbe carino avere un ID&quot;. Ha pensato che il cognome fosse univoco, ma non lo è. Da operatore, devi correggere lo schema. È uno dei problemi di questi strumenti.

Lui dà un titolo, &quot;People data&quot;, descrive il dataset, capisce che `age` è un `integer`, che `birthDate` è `type: date` e ne indovina il formato. Ora ritorno in chat. Posso andare in continuità con `llm -c`. Lui si ricorda il contesto e gli dico: &quot;Ora descrivilo per una tabellina su un sito web&quot;. E mi estrae il Markdown da mettere nel mio portale *open data*.

È possibile salvare *template* e *prompt* per operazioni ricorrenti. Esempio stupido: normalizzare indirizzi stradali scritti in modi diversi. Mi sono costruito un *prompt* verboso in cui gli dico: &quot;Cerca il pattern degli indirizzi, estrai il tipo di strada, l'odonimo, il civico&quot;. Se lo istruite bene, funziona meglio. Queste sono le mie strade, scritte a casaccio. Usando il *template* e passandogli il file `.txt` in *pipe*, mi estrae le informazioni in modo strutturato. Attenzione, è didattica. Con milioni di *record* non si può fare così, bisogna progettare meglio.

Ho quasi finito. Applicazioni: chattare con un video. C'è NotebookLM, ma mi sono fatto una piccola applicazione. Gli do l'URL del video e una domanda. Uso un *template* &quot;handy&quot;, che vuole una risposta sintetica. Posso chiedergli di rispondere in italiano, anche se il video è in un'altra lingua, una magia degli LLM. Vi faccio un esempio: Tim Berners-Lee che parla di Linked Data. Lui dialoga con YouTube, scarica i sottotitoli e mi risponde. I punti chiave sono: &quot;frustrazione con i sistemi attuali&quot;, &quot;i principi&quot;, &quot;l'importanza della connessione&quot;. Tim è sempre Tim.

Perché ve ne parlo? Perché, non essendo uno sviluppatore, ho messo in piedi un po' di codice con un *system prompt*: &quot;Sei un sistema che mi assiste nel rispondere a domande su video YouTube&quot;. Nei *template* posso usare variabili, come `language`. Infatti, posso chiedere la stessa risposta in francese. È un po' più breve, non so perché, ma è in francese.

E fine. Passo la palla ad Andrea, Nelson, Mauro.