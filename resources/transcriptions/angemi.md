Vi dicevo, era giugno 2024. A casa mia, il getto dell'acqua, la portata, comincia a diminuire. Dopo qualche minuto, il getto dell'acqua diventa questo. Dopo qualche minuto ancora, se la portata stava diminuendo, è diventata questa. E il problema, in qualche modo l'avete intuito, è la siccità.

Eh, comincia... allora, la Regione ricomincia a chiedere lo stato di emergenza nazionale. È avvenuto nel 2024, ma è avvenuto anche un mese scarso fa. Si è riproposto identico lo stesso scenario. La Regione, quindi, avanza formalmente la richiesta di dichiarazione per lo stato di emergenza, ed è il 3 aprile del 2024. Quindi, ancora prima della soluzione, cominciano queste riduzioni idriche.

Eh, peccato che in alcuni comuni queste riduzioni sono state così importanti che hanno portato veramente a dinamiche anche sociologiche importanti. Ad esempio, in un comune in provincia di Enna, che è la provincia dalla quale vengo io, l'erogazione idrica avveniva ogni 5 giorni. Immaginate voi: una famiglia di cinque persone con l'acqua a casa ogni 5 giorni. È il comune di... l'unico? No. Il Comune di Gagliano, per dire. Un comune sempre in provincia, uguale: ogni 5 giorni. Ma, in realtà, questa cosa si è riproposta in tantissimi comuni.

E perché vi dicevo che ci sono delle implicazioni sociologiche importanti? Perché i sindaci cominciano ad occupare le sedi, perché queste riduzioni idriche erano legate alla gestione degli invasi. E quindi, una delle dighe importanti per questo territorio, la diga chiamata Ancipa, comincia ad essere occupata dai sindaci. Ma non funziona. Anzi, addirittura alcuni sindaci si incatenano fisicamente alla sede della Protezione Civile regionale, qui a Palermo, per, in qualche modo, denunciare la gestione idrica.

E in tutto ciò, noi ci chiediamo, come sempre: 'E ma dove sono i dati che stanno dietro a queste scelte? Dove sono i dati che stanno dietro alle decisioni della riduzione idrica?'. E vediamo, intanto, se questi dati ci sono e come vengono pubblicati.

Eh, i dati a cui faccio riferimento sono relativi agli invasi a uso potabile, quindi essenzialmente le dighe la cui acqua contenuta all'interno è utilizzata per scopi potabili. Faccio una cosa simile a quella che ha fatto Giorgia, ma Giorgia l'ha fatto meglio, perché lei è Giorgia.

Ehm, ho chiesto a ChatGPT, ma anche a Gemini di Google, insomma... visto che abbiamo capito che è importante comprendere qual è il livello d'acqua contenuto, ad esempio, nell'Ancipa, che rifornisce alcuni comuni, chiedo: 'Cerca il volume invasato ad oggi dalla diga Ancipa, quella in Sicilia. Se non conosci il volume di oggi, almeno dammi quello più recente'. Ehm, ho abilitato però il 'deep search', quindi gli ho chiesto: 'Per favore, cerca in lungo e in largo per il web. Per favore, prova a darmi una risposta'.

Allora, che risposte mi ha dato? Gliel'ho chiesto esattamente stanotte a mezzanotte, non mi ricordo, una cosa simile. Comunque, oggi è il 13 giugno. OpenAI, quindi ChatGPT, mi ha detto: 'Il dato più recente è il primo maggio 2024'. Non è sicuramente oggi, è più di un mese fa. Google mi ha detto che i dati più recenti sono di febbraio del 2024.

Eh, non ne ha azzeccata nessuna delle due, perché, così come diceva Giorgia, i dati in realtà vengono pubblicati da un dipartimento, da una struttura regionale che si chiama 'Autorità di bacino del distretto idrografico della Sicilia', che ha un 'Osservatorio distrettuale permanente sugli utilizzi idrici'. Insomma, c'è questa struttura che pubblica, addirittura giornalmente da poco tempo, i dati relativi agli invasi a uso potabile.

Ora, so che tra il pubblico c'è la Regione con me. Non me ne vogliate se sto per dire... scherzo! No, no, no! Io vi voglio bene, giuro. Ma dobbiamo vedere... scherzo! Quelli che pubblicano così... Però, poi, sicuramente... ma siamo qui per... non me ne vogliate, veramente.

E perché la Regione li pubblica? E li pubblica così, giustamente, no? E io vedo Giorgia Lodi - voi non la vedete - che sta facendo così. E perché? Ma che abbiamo, Giorgia Lodi che fa così? Perché non sarò sicuramente io a dirvi - che sono la persona meno adatta - che i dati aperti sono classificati secondo alcuni livelli, le 5 stelle, eccetera eccetera, perché in questa stanza ci sono tutte le persone che hanno insegnato a me queste cose, quindi non sono io la persona adatta a dirle. E però, chiaramente, il PDF si colloca nel gradino più basso della scala, purtroppo.

E voi mi dite: 'Ah, vabbè, ma se questa è la scala dei dati aperti, allora un PDF è comunque un dato aperto'. Sì. Purtroppo, abbiamo qui anche chi nelle slide scrive: 'Sì, però i dati devono essere disponibili in formato aperto almeno a un livello pari a tre stelle'. E il livello pari a tre stelle, chiaramente, è un CSV, almeno un CSV.

Che si fa, allora? Solitamente, la roba che funziona sempre, insomma, è chiamare Borruso. Segnatevi il numero: 333 491 7592. No, sul serio, giornalmente riceve delle chiamate di questo tipo: 'Ciao, sono un giornalista. Scrivo per Il Sole 24 Ore. Mi sono imbattuto in un PDF di 200 pagine... devo scrivere però un articolo, devo analizzare questi dati, come faccio?'. Borruso è pronto, insomma, a rispondere, perché è presidente dell'associazione onData. In associazione, una cosa che fanno molto spesso è liberare i dati, no? Che sono intrappolati in PDF governativi. Ad esempio, uno su tutti: i dati sugli sbarchi. Vengono pubblicati, forse ancora oggi, con frequenza giornaliera, e noi facciamo l'aggiornamento quindicinale - ok, ogni 15 giorni - loro in modo giornaliero, ma non in formato aperto.

Ma anche la Sicilia, voglio dire, è abituata. E qui c'è Gabriele Scalici, che durante il Covid, insomma, ha estratto tutti i dati relativi all'epidemia da documenti PDF della Regione Siciliana. All'epoca non c'erano ancora gli strumenti, in termini di ChatGPT, LLM, di cui stiamo parlando oggi. Tutto veniva fatto in modo tradizionale. Cos'era, forse un Tesseract che usavi? Eh, sì, sì. Estraeva dati tabulari... prima faceva, diciamo, una lettura dati OCR, poi li rendeva tabulari. Un metodo, se vogliamo, 'a manina', tradizionale. Eravamo nel 2021.

Eh, chiaramente oggi sono cambiate un po' di cose. Forse quello che facevamo in modo tradizionale, e che per carità, continua a funzionare benissimo... anche se a volte con alcuni intoppi. E vediamo se riusciamo a fare una cosa simile, però sfruttando un sistema di intelligenza artificiale, quale può essere questo LLM.

Tutti gli esempi che vi farò vedere a partire da ora sono fatti usando il modello Gemini 1.5 Flash. Attenzione, lo sappiamo, cioè, stiamo utilizzando uno strumento mega potentissimo per fare un task veramente molto banale e che può essere raggiunto anche col metodo tradizionale. Il nostro è un esperimento, per capire se questo strumento può essere adottato anche per estrarre dati a partire da documenti non strutturati, come può essere un documento PDF.

E allora, che dobbiamo fare? Ad esempio, per l'emergenza idrica, potremmo costruirci il nostro algoritmo che cerca i documenti rilevanti, ci estrae i dati e poi li aggiorna, insomma, in continuazione.

Partiamo a identificare i documenti. Questo qui lo facciamo in modo tradizionale, perché è quello più semplice: ci colleghiamo al sito della Regione, ci sono diverse pagine che raccolgono, insomma, questi report in PDF, divisi per anno e per mese, e quindi semplicemente giungiamo al file PDF.

Dopodiché interviene quello che, in qualche modo, forse è lo step più importante, che è quello dell'estrazione dei dati. E qui facciamo tre richieste, facciamo formalmente tre chiamate all'AI di Google. Lui mi dirà: 'Perché? Ce ne basta una'. Ebbene, in realtà ne basta una, però ne facciamo tre perché vogliamo scongiurare al minimo l'eventualità dell'allucinazione. Dobbiamo ridurre al minimo il rischio: facciamo due estrazioni e un sanity check.

Cominciamo con la prima estrazione, che in realtà è la più difficile. Forniamo a Google Gemini la tabella con i dati non strutturati contenente tutti i volumi per ogni diga, insieme a un'anagrafica in CSV che contiene il nome della diga, la localizzazione, le coordinate, la descrizione dell'uso, eccetera eccetera. E il prompt è: 'A partire dai dati presenti non strutturati, estrai i nomi delle dighe, i volumi; addirittura, arricchiscili con il codice identificativo che non è presente nel PDF ma è presente nell'anagrafica. Quindi, fai una sorta di fuzzy match tra i nomi delle dighe nel PDF e nell'anagrafica, che non sempre corrispondono esattamente. Ovviamente, poi fai un join, insomma, ed estrai questo dato così come lo voglio io'.

Abbiamo il primo output, siamo contenti. Lo facciamo una seconda volta, stavolta senza l'anagrafica in CSV. Chiediamo solo: 'Estrai tutti i dati che trovi da questo documento PDF'. E otteniamo il secondo output.

A questo punto, facciamo il sanity check: confrontiamo i due output. Non facciamo il confronto sui nomi delle dighe, perché sappiamo che sicuramente i nomi differiscono, ma facciamo un confronto sui volumi. E l'output che troviamo sulla destra, che ci restituisce l'LLM, non è più un CSV come prima, ma è un report di valutazione in JSON. Ehm, questo JSON contiene solamente tre chiavi: ci dice se la validazione è stata superata, il motivo per cui è stata superata (o non è stata superata) e la data della validazione. E quindi questo qui è il check.

[Voce dal pubblico] Ho una curiosità: perché gli fai fare un controllo tra l'output della prima e l'output della seconda?

[Relatore] Perché la prima è 'guidata'. Cioè, perché nella prima c'è anche l'anagrafica e a volte si confonde tra i dati dei volumi nella diga e i dati che sono presenti nell'anagrafica. Nella seconda chiamata do solamente il PDF, senza anagrafica. Non t'ho convinto?

[Voce dal pubblico] No, no, io quando faccio così, gli faccio fare tre estrazioni.

[Relatore] Ah, invece di farne controllare due. Però, come si diceva prima, non c'è certezza, no?, su come usarli ancora. Semplicemente, faccio controllare le due estrazioni e poi faccio il report di validazione. Sì, sì, solo questo.

E questo processo si ripete per N volte, finché il report non è valido. N volte può essere 3, 4... È capitato che durante la prima iterazione uno dei due output era sbagliato, il report di validazione infatti lo segnalava, eh, e ripartiva da capo. Ed è capitato che dopo un paio di iterazioni - solitamente due, eh, ma ne basta in realtà molto spesso una - riusciamo ad estrarre i dati in modo corretto.

E dopo li aggiorniamo. Li aggiorniamo ogni giorno, perché se questi dati sono pubblicati ogni giorno, li aggiorniamo ogni giorno sul nostro repository di GitHub. E abbiamo una GitHub Action che esegue esattamente tutto quello che vi ho raccontato oggi.

Se non ricordo male... Andrea, ti posso chiedere di fare...? No, no, no, faccio io, perché pensavo fosse collegato. Vediamo se i dati di oggi sono stati estratti. Ehm, vediamo se... questa qui l'ha fatta durante il coffee break... eh... No, non so usare un Mac, scusate! Comunque, si sono estratti i dati, vediamo.

I dati che estraiamo li abbiamo anche documentati. Questo è 'come piace a Borruso'. Ma c'abbiamo anche i metadati in Frictionless, con lo standard del Data Package. Ed essenzialmente è tutto quello che facciamo. Poi abbiamo un bot che notifica sul canale di Open Data Sicilia... anche se poi ci siamo stancati delle notifiche giornaliere e le abbiamo staccate. Però funziona.

E possiamo ottenere delle cose di questo tipo: trasformiamo, così come aveva fatto Gabriele con i dati del Covid, un PDF, se vogliamo, in CSV, in API... E ci chiediamo: questi dati, qualcuno li sta riutilizzando?

Per fortuna, sì. Ad esempio, il Centro Meteorologico Siciliano usa come sorgente 'Dati Sicilia' per le sue valutazioni, i suoi monitoraggi sugli invasi idrici in Sicilia. E anche grazie a un altro membro di Open Data Sicilia che si chiama Giulio Di Chiara, che non so se in realtà oggi c'è. 

Consentitemi questa..

Viva il riuso, ma soprattutto, viva il Borruso!