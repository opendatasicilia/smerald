<!DOCTYPE html>
<html lang="it">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SMERALD Datathon: Esplorare il rapporto tra AI e Open Data</title>
    
    <!-- Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- Google Fonts: Inter -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&display=swap" rel="stylesheet">
    
    <style>
        /* Stile personalizzato per applicare il font Inter */
        body {
            font-family: 'Inter', sans-serif;
        }
        /* Stili per la scrollbar del modal */
        .modal-content::-webkit-scrollbar {
            width: 8px;
        }
        .modal-content::-webkit-scrollbar-track {
            background: #1e293b;
        }
        .modal-content::-webkit-scrollbar-thumb {
            background-color: #38bdf8;
            border-radius: 10px;
            border: 2px solid #1e293b;
        }
        /* Stili per l'accordion */
        .expandable-content {
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.5s ease-in-out;
        }
    </style>
</head>
<body class="bg-gradient-to-br from-slate-900 via-blue-900 to-slate-900 text-white min-h-screen">

    <!-- Contenitore Principale -->
    <div class="container mx-auto p-4 md:p-8 lg:p-12">

        <!-- Header -->
        <header class="text-center mb-16">
            <h1 class="font-extrabold mb-6">
                <span class="text-5xl md:text-7xl text-white block">SMERALD Datathon</span>
                <span class="text-2xl md:text-4xl font-light text-sky-300 tracking-wide block mt-2">Esplorare il rapporto tra AI e Open Data</span>
            </h1>
            <div class="flex flex-col sm:flex-row items-center justify-center gap-6 md:gap-8 text-slate-300 mt-8">
                <div class="flex items-center gap-3">
                    <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6 text-sky-400" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2">
                        <path stroke-linecap="round" stroke-linejoin="round" d="M8 7V3m8 4V3m-9 8h10M5 21h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v12a2 2 0 002 2z" />
                    </svg>
                    <span class="font-medium text-lg">13 e 14 giugno 2025</span>
                </div>
                <div class="flex items-center gap-3">
                    <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6 text-sky-400" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2">
                       <path stroke-linecap="round" stroke-linejoin="round" d="M17.657 16.657L13.414 20.9a1.998 1.998 0 01-2.827 0l-4.244-4.243a8 8 0 1111.314 0z" />
                       <path stroke-linecap="round" stroke-linejoin="round" d="M15 11a3 3 0 11-6 0 3 3 0 016 0z" />
                    </svg>
                    <span class="font-medium text-lg">ITD-CNR di Palermo</span>
                </div>
            </div>
        </header>

        <!-- Sezione Riepilogo e Video Completo -->
        <section class="mb-16 bg-slate-800/60 backdrop-blur-sm border border-blue-800/50 rounded-xl shadow-lg overflow-hidden">
            <div class="grid md:grid-cols-2 items-center">
                <div class="p-8">
                    <h2 class="text-3xl font-bold text-sky-300 mb-4">Rivedi tutto</h2>
                    <p class="text-slate-300 max-w-xl mb-6">
                        L'evento di Palermo ha rappresentato un'importante occasione di confronto sul legame tra open data e AI. Dalle policy nazionali di AgID alle applicazioni pratiche degli LLM, fino al Datathon collaborativo, rivivi due giorni intensi che hanno trasformato le idee in progetti concreti.
                    </p>
                </div>
                <div class="relative p-8 flex items-center justify-center h-full min-h-[250px] bg-cover bg-center" style="background-image: url('https://i0.wp.com/opendatasicilia.it/wp-content/uploads/2025/05/datathon-banner-2.jpg?resize=1110%2C500&ssl=1');">
                    <!-- Overlay per contrasto -->
                    <div class="absolute inset-0 bg-slate-900/70"></div>
                     <a href="https://www.youtube.com/watch?v=I8p-bEY_ago" target="_blank" rel="noopener noreferrer" class="relative z-10 group flex items-center justify-center w-32 h-32 bg-sky-600 rounded-full shadow-lg shadow-sky-500/30 hover:bg-sky-500 transition-all duration-300 transform hover:scale-110">
                        <div class="absolute w-full h-full bg-sky-500 rounded-full animate-ping opacity-30 group-hover:opacity-50"></div>
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-16 w-16 text-white transform transition-transform duration-300 group-hover:scale-110" viewBox="0 0 20 20" fill="currentColor">
                           <path fill-rule="evenodd" d="M10 18a8 8 0 100-16 8 8 0 000 16zM9.555 7.168A1 1 0 008 8.002v3.996a1 1 0 001.555.832l3.196-1.998a1 1 0 000-1.664L9.555 7.168z" clip-rule="evenodd" />
                        </svg>
                    </a>
                </div>
            </div>
        </section>

        <!-- Sezione Interventi -->
        <main>
            <!-- Giorno 1 -->
            <section>
                <h2 class="text-3xl font-bold text-sky-300 border-b-2 border-sky-500/30 pb-3 mb-8">Interventi di Venerdì 13 giugno</h2>
                <div class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-8">
                    
                    <!-- Card Intervento 1: Rotundo -->
                    <div class="bg-slate-800/50 backdrop-blur-sm border border-blue-800/50 rounded-xl p-6 flex flex-col justify-between shadow-lg hover:shadow-sky-500/20 transition-all duration-300">
                        <div class="flex-grow">
                            <h3 class="text-xl font-semibold text-white mb-2">Policy sui dati: dagli open data al Data Governance Act</h3>
                            <p class="text-slate-400 mb-2">Antonio Rotundo (AgID)</p>
                            <p class="text-slate-300 text-sm mb-6">Un'analisi delle nuove policy sui dati, con un focus sul Data Governance Act e le sue implicazioni per la PA e le imprese.</p>
                        </div>
                        <div class="mt-auto pt-4 border-t border-slate-700/60">
                           <div class="flex flex-wrap items-center justify-between gap-3">
                                <div class="flex items-center gap-3">
                                    <a href="https://www.youtube.com/live/I8p-bEY_ago?si=x8IEtG_orHSGzAYg&t=1468" class="text-center bg-sky-600 hover:bg-sky-700 text-white font-semibold py-2 px-3 rounded-lg transition-colors flex items-center justify-center gap-2">
                                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor"><path d="M2 6a2 2 0 012-2h6a2 2 0 012 2v8a2 2 0 01-2 2H4a2 2 0 01-2-2V6zM14.553 7.106A1 1 0 0014 8v4a1 1 0 00.553.894l2 1A1 1 0 0018 13V7a1 1 0 00-1.447-.894l-2 1z" /></svg>
                                        <span>Video</span>
                                    </a>
                                     <a href="resources/slides/rotundo.pdf" class="text-center bg-slate-700 hover:bg-slate-600 text-white font-semibold py-2 px-3 rounded-lg transition-colors flex items-center justify-center gap-2">
                                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M3 17a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zm3.293-7.707a1 1 0 011.414 0L9 10.586V3a1 1 0 112 0v7.586l1.293-1.293a1 1 0 111.414 1.414l-3 3a1 1 0 01-1.414 0l-3-3a1 1 0 010-1.414z" clip-rule="evenodd" /></svg>
                                        <span>Slide</span>
                                    </a>
                                </div>
                                <button class="expand-trigger flex items-center justify-center gap-1.5 text-sm text-sky-400 hover:text-sky-300 font-medium transition-colors py-2 flex-shrink-0">
                                    <span>Approfondisci</span>
                                    <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 transition-transform chevron" viewBox="0 0 20 20" fill="currentColor">
                                        <path fill-rule="evenodd" d="M5.293 7.293a1 1 0 011.414 0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414 0l-4-4a1 1 0 010-1.414z" clip-rule="evenodd" />
                                    </svg>
                                </button>
                            </div>
                            <div class="expandable-content mt-4 space-y-2">
                                <button class="modal-trigger w-full text-left px-3 py-2 text-sm bg-slate-700/50 hover:bg-slate-700 rounded-md text-slate-300 hover:text-white transition-colors" data-title="Riepilogo: Policy sui dati" data-content="Antonio Rotundo di AgID ha illustrato il panorama delle nuove policy sui dati, sottolineando come il successo dell'intelligenza artificiale sia intrinsecamente legato alla disponibilità e alla qualità dei dati. Ha evidenziato che la questione della qualità dei dati, già nota in contesti precedenti, è diventata ancora più lampante con l'avvento dell'IA, poiché risposte imprecise sono inevitabili senza dati di input affidabili. L'Unione Europea ha adottato diverse strategie per i dati, tra cui la direttiva Open Data del 2019 e il Data Governance Act (DGA), che si complementa con essa regolando la circolazione di dati non pubblicabili come aperti (ad esempio, per questioni di protezione dei dati personali o proprietà intellettuale). Ha menzionato anche il Data Act, in vigore dal 2025, che introduce il principio di portabilità dei dati generati da dispositivi connessi a Internet.

                                A livello nazionale, queste direttive sono state recepite, e AgID svolge un ruolo cruciale nell'adozione delle linee guida su vari aspetti, dall'interoperabilità tecnica (come la PDND, che ha permesso milioni di scambi di dati tra PA) agli open data e ai dati di elevato valore. Rotundo ha sottolineato l'importanza di un framework di interoperabilità dettagliato per i dati geografici, come la direttiva INSPIRE, che è stata rivitalizzata dal regolamento sui dati di elevato valore.

                                Un focus significativo è stato posto sulla qualità dei dati, con riferimento alle linee guida Open Data di AgID che identificano caratteristiche di qualità basate su standard ISO, come l'attualità e l'aggiornamento dei dati. Ha ammesso le sfide nella misurazione della qualità e nella mancanza di strumenti adeguati, ma ha ribadito l'impegno di AgID nell'integrare i requisiti di qualità e nel promuovere processi di gestione dei dati unitari per evitare frammentazione. Infine, ha illustrato il ruolo di AgID come autorità nazionale per il DGA, gestendo il riutilizzo dei dati protetti, l'intermediazione dei dati e promuovendo l'altruismo dei dati, oltre a monitorare gli 'spazi di dati' europei.">Leggi il riepilogo</button>
                                <button class="modal-trigger w-full text-left px-3 py-2 text-sm bg-slate-700/50 hover:bg-slate-700 rounded-md text-slate-300 hover:text-white transition-colors" data-title="Trascrizione: Policy sui dati" data-content="Eccoci. Allora, buon pomeriggio a tutti e grazie a Davide, CNR, Open Data Sicilia per l'invito. È la prima volta, mi sa, che vengo a un evento di Open Data Sicilia e mi fa molto piacere, chiaramente, perché ci conosciamo da tanto tempo e condividiamo diverse cose, insomma.

                                A me tocca il compito di fare una panoramica sulle policy, quindi sicuramente le cose più accattivanti arriveranno dopo, però diciamo che può essere utile fare anche insieme questo quadro, perché poi, come sappiamo, l'interoperabilità è basata su regole e quindi conoscere le regole che ci aiutano a garantire interoperabilità, Open Data, intelligenza artificiale, eccetera, può essere utile a tutti.

                                Allora, io, vabbè, lavoro in AgID, nell'area che si chiama 'Interoperabilità, Dati e Accessibilità'. Come sapete, AgID, tra le altre cose, fa anche linee guida su diversi aspetti (poi lo vedremo), per cui fa le regole che poi sono alla base di tutte le attività delle pubbliche amministrazioni, che devono mettere a disposizione i dati, devono realizzare servizi e API e devono implementare, ormai, anche l'intelligenza artificiale.

                                Quindi, allora, lo sappiamo, sicuramente non ve lo devo dire, ma da quando stiamo parlando in maniera più importante, diciamo, ormai da qualche anno, di intelligenza artificiale, automaticamente parliamo di dati e di qualità dei dati. Nel senso, è come se con l'intelligenza artificiale avessimo scoperto l'importanza dei dati, l'importanza di avere dati di qualità, perché comunque ci siamo scontrati direttamente. Lo sapevamo già prima, ma è probabilmente più lampante in questo ultimo periodo: abbiamo visto che se non ci sono dati di qualità, alla fine le risposte che l'intelligenza artificiale ci dà sono quantomeno inesatte, diciamo. E quindi abbiamo scoperto che effettivamente quello che dicevamo da anni, quindi se io ho dati in ingresso che non sono puliti, non sono di qualità, è chiaro che qualsiasi cosa faccia (servizi digitali, applicazioni, eccetera) chiaramente non sarà efficace ed efficiente.

                                Ed effettivamente, questa grande ondata di attività sull'intelligenza artificiale avrà successo, e lo dice anche il Parlamento Europeo direttamente, solo se le strategie sui dati, e quindi tutte le attività sui dati, avranno successo. Cioè, se continueremo a pubblicare dati che non sono di qualità, è chiaro che anche l'intelligenza artificiale è destinata a non produrre i risultati che ci auspichiamo. Ed effettivamente l'AI Act lo dice chiaramente. Lo dice chiaramente: il regolamento sull'intelligenza artificiale si rivolge soprattutto a sistemi ad alto rischio, però il discorso è valido per tutti i sistemi di intelligenza artificiale, e cioè che servono adeguate pratiche di governance e gestione dei dati perché ci sia a disposizione una grande disponibilità di dati e dati di qualità. Ora, quello che dico io non è che in altri casi, senza intelligenza artificiale, non debbano essere garantite queste condizioni, perché se noi facciamo un servizio pubblico digitale ci servono grandi quantità di dati e ci servono dati di qualità, così per qualsiasi altra cosa. Quindi sono cose scontate che stanno prendendo di nuovo e forse l'intelligenza artificiale ci può aiutare a recuperare molto tempo che abbiamo forse perso negli anni passati, per arrivare, si spera, a queste grazie a queste condizioni.

                                Chiaramente, da diversi anni, prima del regolamento sull'intelligenza artificiale, quindi prima che nascesse tutta questa discussione e queste attività degli ultimi anni, in realtà l'Europa, la Commissione Europea e, di conseguenza, anche i vari Stati membri, stanno lavorando da diversi anni sulle strategie per i dati. C'è la grossa fetta degli Open Data. Sapete, nel 2019 c'è stata la nuova direttiva – lo vedremo meglio dopo – che, lo dicevamo con Andrea recentemente, finalmente si chiama 'direttiva sull'apertura dei dati'. Prima era 'Public Sector Information', che era una cosa molto più generica, ma non rendeva subito l'idea, ecco. Però, l'approccio che la Commissione Europea sta adottando è che si deve aumentare l'offerta dei dati, non solo dati aperti: anche tutti gli altri dati che non possono essere aperti per diversi motivi devono comunque circolare, in modo tale che quei dati possano creare valore così come lo possono fare gli Open Data.

                                E quindi è intervenuto il Data Governance Act, che praticamente è complementare alla disciplina Open Data e quindi riguarda quei dati che non possono essere pubblicati come dati aperti perché, per esempio, ci sono questioni di protezione dei dati personali, ci sono questioni di segreto commerciale, quindi c'è un impatto sulle attività commerciali, oppure c'è la proprietà intellettuale. Nonostante questa limitazione, il Data Governance Act dice che gli enti pubblici devono rendere comunque disponibili questi dati. Chiaramente, non saranno disponibili a tutti e per tutti gli scopi come gli Open Data, ma si innesca un processo di, diciamo, scambio sicuro e affidabile e con attori che si conoscono, perché chiaramente c'è chi fornisce il dato e c'è chi lo deve riutilizzare, per cui l'amministrazione, in questo caso, sa effettivamente chi lo riutilizza e, se lo può riutilizzare, lo fa riutilizzare.

                                Collegato al meccanismo, diciamo, e all'attività di trattamento in ambiente sicuro, o meglio, il Data Governance Act fornisce anche l'inquadramento normativo per una serie di spazi di dati che si stanno costruendo a livello europeo. Comincia a esserci qualche esperienza anche in Italia. Spazi di dati che sono dei veri e propri ecosistemi che mettono insieme soggetti non solo pubblici ma anche privati (perché in agricoltura c'è l'amministrazione pubblica che produce i dati, ma ci può essere anche il singolo contadino che produce il dato) e quindi si devono mettere, diciamo, in relazione in modo tale che, nell'ambito di questo spazio di dati, questi dati, appunto, possano circolare e possano essere scambiati, anche qui, in modo sicuro, affidabile e in modo che seguano standard e regole prestabilite.

                                E poi, in ultimo, entrerà in vigore a settembre 2025 il Data Act. Il Data Act riguarda tutti quei dati che vengono generati attraverso dispositivi connessi a Internet, quindi dispositivi IoT. Per cui, fino ad oggi, o fino a quando non andrà in vigore il Data Act, praticamente, tutti i dati che sono prodotti da questi dispositivi solo il produttore li utilizza e decide come utilizzarli, chi li può utilizzare, eccetera. Dall'entrata in vigore si introduce un principio che si chiama proprio 'portabilità dei dati', nel senso che, una volta che viene dismesso quel dispositivo, è l'utente a decidere che fine dovranno fare quei dati, chi li può utilizzare, quindi chi può creare valore da quei dati. Per l'amministrazione pubblica – visto che a noi come AgID interessa anche, ovviamente, l'amministrazione pubblica – in questo regolamento c'è un meccanismo che fa sì che la pubblica amministrazione possa obbligare i privati a fornire questi dati nei casi di emergenza, per esempio terremoto, alluvione e quant'altro. È chiaro che all'amministrazione pubblica è utilissimo sapere subito, nel più breve tempo possibile, quali sono, per esempio, la localizzazione dei vari cittadini, dove sono dislocati, e quant'altro. In quel caso, l'amministrazione pubblica... e quindi si ha un processo, se vogliamo, perché fino ad oggi c'è stata una richiesta di dati dall'amministrazione pubblica verso il privato. È chiaro che in questo caso si inverte il flusso, quindi è il privato che contribuisce con l'amministrazione pubblica a rendere disponibili questi dati.

                                Questo è il quadro che si sta delineando. Poi vedremo che si sta ragionando già a un'evoluzione a livello europeo rispetto a questa strategia. Chiaramente, c'è la corrispondenza di questo framework europeo a livello nazionale, per cui, vabbè, la direttiva Open Data, chiaramente, è stata recepita; adesso vediamo un po' più nel dettaglio la guida prevista. Il Data Governance Act, chiaramente, non va recepito perché non è una direttiva, è un regolamento, quindi è direttamente applicabile, però richiedeva che fosse identificata un'autorità nazionale (che potesse essere anche più di un'autorità nazionale) che potesse presidiare tutti questi aspetti del Data Governance Act: quindi la condivisione dei dati protetti, i servizi di intermediazione dei dati, l'altruismo dei dati, e poi ne parleremo.

                                E questo, non voglio adesso fare il giurista – lo dicevo pure l'altra volta – però forse un po' ci tocca. Questo è il quadro, diciamo, regolatorio per quanto riguarda gli Open Data. Continua a essere valido in Italia un decreto legislativo che è del 2006, ma è stato aggiornato a seguito della direttiva del 2019. E questo decreto prevedeva che AgID adottasse delle linee guida sugli Open Data, che quindi sono dei documenti vincolanti per l'amministrazione pubblica, ma non solo, perché la direttiva Open Data ha ampliato, diciamo, l'obbligo per l'apertura dei dati non solo alle pubbliche amministrazioni, ma anche a imprese pubbliche e private che esercitano un servizio pubblico. Per cui, se abbiamo l'azienda di mobilità, che è comunque una società privata, è comunque obbligata a rendere disponibili i dati come dati aperti.

                                Prevedeva, dicevo, delle linee guida (ne parleremo dopo). In realtà, poi, da diversi anni – parliamo del 2007, quindi tanti anni fa – su una particolare tipologia di dati, che sono i dati geografici (su cui poi ci sarà un focus nella presentazione successiva), era stato già definito, ed è stato definito fino ad oggi, un framework di interoperabilità molto chiaro e completo. Nel senso, non esistono altre tipologie di dati che abbiano un framework di interoperabilità di dominio così dettagliato e così specifico. Tant'è vero che questo – e mi riferisco alla direttiva cosiddetta Inspire, che è la direttiva che regola, diciamo, le infrastrutture di dati territoriali – questo framework viene preso anche come esempio in altri domini, perché praticamente questo framework regola tutti gli aspetti che riguardano i dati, in questo caso i dati territoriali: dai metadati (quindi definisce un profilo di metadati), definisce per ciascuna categoria di dati delle specifiche tecniche (quindi ci sono delle regole condivise a livello europeo su tutti i temi che sono identificati in Inspire) e definisce i servizi di rete, quali devono essere e come devono essere sviluppati. Ovviamente è tutto basato su standard internazionali, sia ISO che OGC, e definisce anche le regole di scambio dei dati.

                                Ora, la direttiva Open Data... e qui si è creato praticamente il legame, perché fino a qualche anno fa i due mondi viaggiavano separati, diciamo. Quindi c'era il mondo Open Data da una parte e il mondo dei dati territoriali dall'altro. Tant'è vero che nella direttiva Inspire non si parla di apertura di dati, si parla solo di scambio di dati tra pubbliche amministrazioni, per cui c'era una certa, diciamo, difformità anche sul riutilizzo dei dati in questi due ambiti. Con il regolamento sui cosiddetti dati di elevato valore si crea un ponte, diciamo, tra le due discipline. Quindi, la direttiva Open Data individua, anche tra le altre cose, una serie di tipologie di dati che sono importanti per l'utilizzo, perché creano valore non solo economico, ma anche sociale e ambientale. Per esempio, esplicitamente la direttiva cita i dati dinamici, dà delle indicazioni sui dati dinamici, dà delle indicazioni sui dati di ricerca (visto che qua siamo al CNR, è uno degli ambiti che la direttiva Open Data ritiene importante ed essenziale) e poi definisce, crea diciamo, una nuova categorizzazione di dati che sono i 'dati di elevato valore', proprio perché, sulla base di studi che la stessa Commissione ha effettuato negli anni scorsi, questi dati, se riutilizzati – ed è questa la definizione esplicita data dalla direttiva – possono creare anche esplicitamente posti di lavoro e quindi avere un beneficio economico, ma anche sociale e ambientale.

                                Per queste sei categorie, poi, la direttiva demandava a un regolamento l'identificazione puntuale di serie di dati per ciascuna di queste categorie, cosa che è avvenuta con il Regolamento 2023/138. Se andate a vedere, o se conoscete già il regolamento, per ognuna delle sei categorie che la direttiva Open Data individua, vengono puntualmente indicate quali sono le serie di dati da rendere disponibili, secondo alcuni requisiti: quindi dati aperti, attraverso API, attraverso download in blocco, attraverso alcune regole di interoperabilità. E molti di questi dati sono, appunto, dati territoriali. E il regolamento stesso non è che va a inventarsi nuove regole: dice esplicitamente, visto che abbiamo già delle regole codificate da diversi anni su questi tipi di dati, chiaramente richiama quelle regole e dice esplicitamente che si applicano le regole su dati, servizi e metadati della direttiva Inspire per quei dati territoriali che ricadono nelle serie di dati.

                                A livello nazionale, sia nelle linee guida sia nel Piano Triennale (il documento che, diciamo, definisce obiettivi e risultati attesi per tutte le pubbliche amministrazioni per tutti i temi che riguardano la trasformazione digitale) e quindi, dicevo, sia nelle linee guida sia nel Piano Triennale era previsto che, per quanto riguarda i dati di elevato valore, si adottasse una guida operativa. Quindi non è a livello di linea guida vincolante, ma una guida operativa che è stata pensata, da una parte, per fare lo stato dell'arte, cioè: viste le serie di dati definite dal regolamento, a che punto siamo? Perché, chiaramente, derivando alcuni dati dalla direttiva Inspire, c'era e c'è la possibilità che alcuni dati siano già disponibili secondo la direttiva Inspire. E, come dicono molte volte le amministrazioni, se avessimo lavorato negli anni scorsi, quindi dal 2007 ad oggi, facendo in modo che tutti i dati che ricadono nell'ambito di Inspire fossero già allineati a quelle indicazioni (quindi i metadati, e sui metadati diciamo siamo abbastanza allineati, ma anche a livello di conformità dei dati o a livello di conformità dei servizi), a quest'ora il regolamento sarebbe, fatemi passare il termine, una passeggiata. Perché io ho già le API (essendo i servizi di INSPIRE considerati API), ho già i dati allineati alle specifiche tecniche e quindi già rispettano e comprendono gli attributi richiesti dal regolamento e, chiaramente, li ho già metadatati.

                                Chiaramente, come per tutte le applicazioni di norme, c'è una situazione abbastanza eterogenea, per cui non tutte le diverse amministrazioni l'hanno fatto e quindi adesso ci ritroviamo a inseguire, se volete, il discorso del regolamento. Il rapporto tra il regolamento sui dati di elevato valore e Inspire è lo stesso che c'è tra intelligenza artificiale e dati, nel senso che alla fine il regolamento ha rivitalizzato Inspire. Nel senso che ha ripreso le regole Inspire, ha detto 'applicate le regole Inspire', ma se lo avessimo fatto prima è chiaro che saremmo già avvantaggiati. Stessa cosa l'intelligenza artificiale: ma se avessimo già prodotto nella pubblica amministrazione dati di qualità, è chiaro che avremmo una quantità di dati utilizzabile, diciamo, molto maggiore di quanto effettivamente abbiamo.

                                A livello nazionale, poi, nel Codice dell'Amministrazione Digitale, relativamente a Inspire, c'è la definizione di un framework di interoperabilità su cui, ripeto, parlerà poi in un'altra presentazione. E possiamo andare... Ah, in basso. Anzi, è un test, scusate ma... facilità. Grazie.

                                Quindi, chiaramente, ero partito dal discorso intelligenza artificiale e dati. Noi stiamo lavorando in questo periodo sulle linee guida per l'adozione dell'intelligenza artificiale nelle pubbliche amministrazioni. C'è stata la consultazione pubblica, sono arrivati tantissimi commenti, circa un migliaio, e quindi li stiamo elaborando per poter avere un testo da far proseguire nell'iter, che prevede l'articolo 72 del CAD. Dicevamo all'inizio: il successo dell'IA si avrà solo se le strategie sui dati hanno successo. E quindi, siccome nelle linee guida sull'intelligenza artificiale c'è una sezione dedicata, ovviamente, alla gestione dei dati, lì diciamo chiaramente che le amministrazioni pubbliche sono chiamate ad attuare, se non lo avessero già fatto, tutte le policy che sono state definite in questi anni, anche a livello di linee guida sui dati.

                                AgID in questi anni ne ha prodotte diverse, per cui si va dall'interoperabilità tecnica, le API su cui si basa poi la PDND (e anche qui c'è una linea guida sulla PDND). Per cui, al momento, sulla PDND – l'altro giorno, diciamo, c'erano stati 640 milioni di scambi di dati fra pubbliche amministrazioni – è chiaro che questo è possibile perché ci sono regole condivise, perché le amministrazioni seguono le regole di interoperabilità tecnica, quindi espongono API che sono basate su regole condivise, e quindi, chiaramente, funziona lo scambio di dati. Ma poi ci sono, lo citavo prima, le linee guida Open Data e quindi il documento dà l'indicazione per tutto il processo di produzione e pubblicazione degli Open Data, per cui si va dall'analisi del patrimonio interno dell'amministrazione, della selezione dei dati da aprire, alla bonifica, alla valutazione, all'analisi della qualità, alla scelta dello strumento per la pubblicazione e quant'altro. E poi c'è la guida operativa sia sui dati di elevato valore che citavo prima, oltre ad alcune regole tecniche specifiche, anche qui sui dati territoriali che, comunque, in nella maggior parte dei casi sono allineati a Inspire.

                                Questo per dire, come dicevo prima, è chiaro che se seguiamo, diciamo, le regole che insieme decidiamo (perché tutte queste linee guida vengono fatte con un iter che è abbastanza partecipato dalle amministrazioni, perché vengono coinvolte, è partecipato anche da tutti, diciamo, gli utenti esterni, chiamiamoli esterni, perché comunque c'è un periodo di consultazione pubblica) e quindi queste sono il frutto di tutto questo processo partecipativo. E quindi le linee guida Open Data hanno definito, dicevo, una serie di requisiti e raccomandazioni per l'apertura dei dati. Tra le altre cose, siccome dicevamo prima che l'AI Act dice che bisogna mettere in campo adeguate pratiche di gestione dei dati per avere disponibilità di dati e dati di qualità, chiaramente, quando parliamo di qualità dobbiamo intenderci su cosa è la qualità. Per questo, nelle linee guida Open Data, già vengono identificate alcune caratteristiche di qualità che, in realtà, sono state riprese da documenti che sono stati pubblicati dal 2013, se non ricordo male. Quindi sono più di 10, 11 anni, insomma 12 anni che stiamo dicendo: per la qualità, considerate queste caratteristiche. Ovviamente, sono sempre basate su standard ISO.

                                E qual è la difficoltà? Uno, che in molti casi, e su questo si sta lavorando, chiaramente, le amministrazioni hanno bisogno di strumenti per la misura della qualità. Cioè, io dico la coerenza semantica: è chiaro che io devo avere un'ontologia, uno schema, un modello dati di riferimento per misurare se effettivamente quel dato è coerente. Ora si sta lavorando, e forse lo conoscete, conoscete sicuramente `schema.gov.it`, che è il catalogo delle ontologie, degli schemi di dati, dei vocabolari controllati. È chiaro che è un lavoro in progress, per cui non ci sono al momento ontologie per tutte le tipologie di dati, quindi man mano sta crescendo e quello è uno strumento, per esempio, che può essere utile come base, come supporto per strumenti per la misurazione della qualità.

                                Quindi, in genere, una delle critiche per le linee guida è che non sono obbligatorie. La sezione sulla qualità non è obbligatoria. Diciamo che in parte è voluta proprio per questo, perché se non dai strumenti alle amministrazioni su come misurare questa qualità, è chiaro che non puoi pretendere che l'amministrazione... ovviamente devi pretendere che lo faccia, ma che sia un percorso che poi arrivi anche all'obbligo. Questo che vuol dire? Che dall'anno scorso abbiamo cominciato a inserire sul Piano Triennale la conformità a queste caratteristiche di qualità. Nel senso, abbiamo iniziato dalla più facile, diciamo, che è l'attualità, l'aggiornamento dei dati. Come sapete, nei metadati si può indicare la frequenza di aggiornamento, si può indicare la data di ultima modifica. È chiaro che se queste due informazioni non 'matchano', perché se ho messo che è mensile, però ho una data di ultima modifica di un anno fa, è chiaro che il dato non è aggiornato. Quindi l'anno scorso l'abbiamo inserita e quindi dall'anno scorso cominciamo a misurare. Ovviamente lo possiamo fare sulla base dei dati documentati su `dati.gov.it`, non abbiamo altri strumenti per il momento. E ovviamente i dati documentati su `dati.gov.it` non sono tutti i dati della pubblica amministrazione, ma sono una piccolissima parte di tutti i dati. Noi 'arvestiamo' quelli. Tutto il patrimonio della pubblica amministrazione non lo conosciamo. Lo dicevo forse un'altra volta, qualcuno ci chiedeva: 'Ma perché non mettiamo la percentuale di dati aperti rispetto ai dati totali?'. I dati totali chi li conosce? E quindi non possiamo farlo.

                                Quindi questo per dire: le linee guida Open Data hanno definito, se volete, un modello di qualità che sono quattro caratteristiche delle 15 definite dall'ISO 25012. Poi, in aggiunta, io ho messo accessibilità e riservatezza, perché queste sono due caratteristiche di qualità che derivano da norme obbligatorie: l'accessibilità per la Legge Stanca (e quindi tutte le regole che sono succedute, diciamo, alla Legge Stanca) e la riservatezza per il GDPR. Quindi quelle due, anche se non sono indicate esplicitamente nel modello di qualità, ma questo è scritto anche nelle linee guida, è chiaro che sono automaticamente da garantire, perché ci sono altre norme che lo dicono.

                                Nelle linee guida sull'intelligenza artificiale, che abbiamo fatto un po' come il regolamento sui dati di elevato valore, non è che abbiamo buttato le regole che c'erano prima, e quindi abbiamo costruito e definito altre regole. Abbiamo detto: 'Tutte le regole che ci sono su Open Data, ma anche su dati che non siano Open Data, vanno seguite'. Ovviamente, per quanto riguarda la qualità, abbiamo integrato il modello di qualità con altre caratteristiche che possono essere utili nell'ambito dei sistemi di intelligenza artificiale. Quindi diciamo è un modello esteso rispetto a quello delle linee guida Open Data, chiaramente facendo riferimento poi ai requisiti dell'AI Act. Per questo io li ho riportati: chiaramente, tutti questi poi vengono rappresentati da quelle caratteristiche di qualità che sono degli standard ISO. Perché qua il 25012 è stato aggiunto il 25059, che è quello specifico per la misura di qualità per l'intelligenza artificiale.

                                La stessa cosa sul processo di gestione dei dati. C'è il rischio, e poi lo dirò anche dopo, che si creino nelle pubbliche amministrazioni tante strutture, per cui io ho la struttura che si occupa di Open Data, ho la struttura che si occupa dei dati territoriali, ho la struttura che adesso si deve occupare dei dati per l'intelligenza artificiale... Così come i processi: quando c'è – perché non è detto che ci sia – un processo di gestione dei dati, c'è il rischio che si facciano processi diversi e che non ci sia una gestione unitaria e complessiva dei dati a livello di amministrazione. Allora, siccome avevamo già definito un processo di gestione dati nelle linee guida Open Data, abbiamo analizzato gli aspetti di governance e gestione che sono riportati all'articolo 10 delle regole sull'intelligenza artificiale, abbiamo analizzato il ciclo di vita del dato così come definito dagli standard ISO e abbiamo cercato, abbiamo definito un processo di gestione dei dati che sia unitario. Cioè, ci sono alcune fasi che non sono specifiche per Open Data o per dati per l'intelligenza artificiale, ma sono comuni a tutti. Per cui, perché io devo avviare processi diversi quando ovviamente devo e posso ottimizzare, anche perché poi le risorse non sono infinite (per chi è nella pubblica amministrazione soprattutto, lo sa)? Perché devo avviare diversi processi con il rischio che magari il dato creato da un certo dipartimento di un'amministrazione poi non possa essere riutilizzato da un'altra, perché magari, e questo avviene, seguono anche standard diversi? Per cui abbiamo rivisto, diciamo, il processo di gestione dati, integrandolo ovviamente con alcune attività specifiche per i dati nell'ambito dei sistemi di intelligenza artificiale.

                                (Antonio, c'è un motivo per cui hai scelto dei colori diversi per alcuni blocchi?)
                                Perché quelli grigi sono specifici per l'intelligenza artificiale, quindi sono operazioni che vengono richieste per dati che possono essere utilizzati per l'addestramento di sistemi di intelligenza artificiale. Cioè, l'analisi della qualità io non la devo fare solo per i dati dell'intelligenza artificiale, la devo fare per gli Open Data, la devo fare per i dati territoriali, la devo fare per tutte le tipologie di dati. Così come la pianificazione e raccolta. Anzi, in fase di pianificazione io posso lì, sulla base dei requisiti che ho, delle esigenze, definire quali dati produrre e quindi rendere come Open Data, che comunque possono essere utilizzati anche nell'ambito dell'intelligenza artificiale, e quant'altro.

                                E chiaramente questo lo riporto perché tra le attività che AgID fa, abbiamo detto, ci sono le regole tecniche, però voi sapete che gestisce anche due portali nazionali di dati. E quello che ormai dico spesso, anche soprattutto ultimamente, è che non è sufficiente rendere disponibili i dati, è necessario anche farli conoscere. Cioè, l'utente, se tu pubblichi un dato, come fa a saperlo se non va a girare tutti i siti delle pubbliche amministrazioni? Vediamo, se la pubblica amministrazione... c'è uno strumento che è il catalogo dei dati, che serve proprio a quello: evitare all'utente di girare 10.000 siti delle pubbliche amministrazioni. È chiaro che per fare questo, e perché sia, diciamo, utile, le pubbliche amministrazioni devono documentare i propri dati in questi cataloghi. Chiaramente, anche qui, è un lavoro in progress. Poi si amplifica comunque la conoscibilità, perché poi da questi portali nazionali vengono pubblicati sul portale europeo o sui portali europei (c'è pure il geoportale Inspire) e quindi l'utente, a prescindere dal punto di avvio della ricerca, perché uno può anche non conoscere il repertorio, può anche non conoscere `dati.gov.it`, magari si imbatte nell'European Data Portal e da lì riesce ad arrivare al dato. Così, viceversa, io da `dati.gov.it` posso arrivare a un dato geografico perché è anche aperto ed è documentato in entrambi i cataloghi.

                                Velocemente, scusate qualche minuto. Lo dicevo all'inizio: il Data Governance Act si riferisce a tutti gli altri dati che non possono essere aperti. Come dicevo, AgID è stata individuata come autorità nazionale per tutti i compiti previsti nel Data Governance Act. Gli aspetti coperti dal Governance Act, li citavo prima, sono il riutilizzo di dati protetti (quindi tutti quei dati della pubblica amministrazione che non possono essere pubblicati come dati aperti devono comunque essere resi disponibili, ovviamente in ambiente sicuro, eccetera, eccetera, anonimizzati quando serve, eccetera, però devono essere resi disponibili). È chiaro che in questo caso non sono resi disponibili a tutti, ma a utenti conosciuti. Anche per questi dati, su `dati.gov.it` ci sarà una sezione dedicata, così come c'è attualmente sul portale europeo, perché viene richiesto proprio che nel Data Governance Act ci sia uno sportello unico in cui l'utente possa andare a vedere quali dati sono disponibili, anche se, ovviamente, a differenza degli Open Data, non li può scaricare. Quindi va a vedere quali dati sono disponibili, anche protetti, e quindi poi, se gli serve uno di questi dati protetti, può rivolgersi all'amministrazione che ne è titolare.

                                L'intermediazione di dati: tutti i dati, anche delle imprese, possono essere comunque resi disponibili e viene introdotta proprio la figura del fornitore di servizi di intermediazione, cioè proprio una figura intermedia che fa da tramite tra chi produce il dato e chi lo vuole riutilizzare. Si parla di un vero e proprio marketplace dei dati, quindi ovviamente deve guadagnare l'intermediario e quindi si crea anche un mercato. Questo consente anche alle piccole aziende... adesso soprattutto i grandi player utilizzano e hanno la forza, diciamo, di riutilizzare i dati. In questo caso anche le piccole aziende possono intervenire in questo processo.

                                E poi c'è il principio, diciamo bellissimo, dell'altruismo dei dati, che però non so quanto sarà applicato. Cioè, noi lo stiamo applicando come Open Data Sicilia, attivisti, persone... comunque ci sono, intendo anche te, come organizzazioni che non sono obbligate per legge, per norma, a rendere disponibili dati, che, diciamo, a livello volontaristico rendono disponibili comunque i dati. A livello europeo, al momento, una sola organizzazione si è registrata (perché bisogna registrarsi in Italia, presso AgID) e solo un'organizzazione si è registrata come 'organizzazione per l'altruismo dei dati'.

                                Finisco dicendo che avevo citato pure i data space, quindi gli spazi di dati, che sono ecosistemi, diciamo, in cui intervengono diversi attori e sono stati individuati 14 spazi di dati: agricoltura, pubblica amministrazione, dati sanitari, eccetera. Il primo, diciamo, spazio di dati su cui è stato adottato un regolamento, quindi delle regole, delle norme vere e proprie, è il regolamento dello Spazio Europeo dei Dati Sanitari. Questo ha una forte connessione con il Data Governance Act e lì c'è un esempio. Per esempio, si prevede un catalogo dei dati. Ovviamente, i dati sanitari non possono essere utilizzati da tutti, ma ci sono utilizzi primari (quindi tra strutture sanitarie, per esempio, che utilizzano i dati di altre strutture sanitarie) o un utilizzo secondario, cioè dati che possono essere utilizzati anche da altri utenti in generale e che li utilizzano per altri scopi, anche di ricerca, di analisi, eccetera. È previsto un catalogo. Questo catalogo, ovviamente, dovrà essere in relazione con quello che è lo sportello unico che sarà presente su `dati.gov.it`. Come vedete, tutto è collegato e tutto si tiene.

                                Chiaramente, quell'esigenza che dicevo all'inizio, cioè l'esigenza di avere dati affidabili, sicuri, ben organizzati, di qualità, è un'esigenza chiaramente a livello europeo, tanto che nella seconda metà del 2025 è prevista una nuova strategia sui dati dell'Unione che possa garantire e accelerare parecchio questa, diciamo, attività sulla qualità dei dati. Fino al 18 luglio, se volete, c'è una consultazione pubblica (poi c'è il link, diciamo, sulla slide): si possono, chiaramente, fornire osservazioni e indicazioni su che cosa ci si aspetterebbe da una strategia dei dati dell'Unione Europea, ovviamente ai fini della disponibilità dei dati per l'intelligenza artificiale. Grazie.">Leggi la trascrizione</button>
                            </div>
                        </div>
                    </div>
                    
                    <!-- Card Intervento 2: Donnaloia -->
                    <div class="bg-slate-800/50 backdrop-blur-sm border border-blue-800/50 rounded-xl p-6 flex flex-col justify-between shadow-lg hover:shadow-sky-500/20 transition-all duration-300">
                        <div class="flex-grow">
                            <h3 class="text-xl font-semibold text-white mb-2">Dati geografici e regolazione tecnica condotta da AgID</h3>
                            <p class="text-slate-400 mb-2">Leonardo Donnaloia (AgID)</p>
                            <p class="text-slate-300 text-sm mb-6">Focus sul ruolo di AgID nella standardizzazione dei dati geografici e presentazione delle specifiche tecniche per i dati geospaziali.</p>
                        </div>
                        <div class="mt-auto pt-4 border-t border-slate-700/60">
                           <div class="flex flex-wrap items-center justify-between gap-3">
                                <div class="flex items-center gap-3">
                                    <a href="https://www.youtube.com/live/I8p-bEY_ago?si=SrBPrWISe1y_vdp4&t=4490" class="text-center bg-sky-600 hover:bg-sky-700 text-white font-semibold py-2 px-3 rounded-lg transition-colors flex items-center justify-center gap-2">
                                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor"><path d="M2 6a2 2 0 012-2h6a2 2 0 012 2v8a2 2 0 01-2 2H4a2 2 0 01-2-2V6zM14.553 7.106A1 1 0 0014 8v4a1 1 0 00.553.894l2 1A1 1 0 0018 13V7a1 1 0 00-1.447-.894l-2 1z" /></svg>
                                        <span>Video</span>
                                    </a>
                                     <a href="resources/slides/donnaloia.pdf" class="text-center bg-slate-700 hover:bg-slate-600 text-white font-semibold py-2 px-3 rounded-lg transition-colors flex items-center justify-center gap-2">
                                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M3 17a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zm3.293-7.707a1 1 0 011.414 0L9 10.586V3a1 1 0 112 0v7.586l1.293-1.293a1 1 0 111.414 1.414l-3 3a1 1 0 01-1.414 0l-3-3a1 1 0 010-1.414z" clip-rule="evenodd" /></svg>
                                        <span>Slide</span>
                                    </a>
                                </div>
                                <button class="expand-trigger flex items-center justify-center gap-1.5 text-sm text-sky-400 hover:text-sky-300 font-medium transition-colors py-2 flex-shrink-0">
                                    <span>Approfondisci</span>
                                    <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 transition-transform chevron" viewBox="0 0 20 20" fill="currentColor">
                                        <path fill-rule="evenodd" d="M5.293 7.293a1 1 0 011.414 0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414 0l-4-4a1 1 0 010-1.414z" clip-rule="evenodd" />
                                    </svg>
                                </button>
                            </div>
                            <div class="expandable-content mt-4 space-y-2">
                                <button class="modal-trigger w-full text-left px-3 py-2 text-sm bg-slate-700/50 hover:bg-slate-700 rounded-md text-slate-300 hover:text-white transition-colors" data-title="Riepilogo: Dati geografici" data-content="Leonardo Donnaloia ha descritto il ruolo di AgID nella standardizzazione dei dati geografici e nella definizione delle specifiche tecniche per i dati geospaziali. L'obiettivo principale è valorizzare il patrimonio informativo pubblico e definire modelli di dati interoperabili, considerando l'evoluzione dei sistemi informativi territoriali con l'intelligenza artificiale. L'IA richiede dati sempre più dinamici e strutturati, rendendo essenziale l'adozione di regole e modelli riconosciuti.

                                Il fulcro del lavoro di AgID in questo ambito è il supporto alle pubbliche amministrazioni per l'implementazione dei 'database geotopografici' (DBGT), una struttura consolidata dal 2011 per la rappresentazione dei dati territoriali. Questi database sono organizzati in strati, temi e classi, permettendo l'aggiunta controllata di contenuti tematici specifici. Donnaloia ha fornito esempi di progetti in cui AgID ha supportato la definizione di data model tematici interoperabili 'by design', come il PEL IP per l'illuminazione pubblica e il SINFI per la banda ultralarga.

                                Ha evidenziato che i DBGT contengono intrinsecamente molti dei dati considerati di elevato valore (High Value Datasets - HVDs) a livello europeo, specialmente quelli con componente geografica. Sebbene molte regioni abbiano già dati conformi, la sfida principale rimane l'aggiornamento regolare e la piena conformità alle specifiche INSPIRE, richieste dal regolamento HVD. La sua presentazione ha messo in luce come un'adeguata gestione e strutturazione dei dati geografici non solo risponda a obblighi normativi, ma abiliti anche nuove possibilità di analisi e riutilizzo per l'IA e la gestione degli asset territoriali.">Leggi il riepilogo</button>
                                <button class="modal-trigger w-full text-left px-3 py-2 text-sm bg-slate-700/50 hover:bg-slate-700 rounded-md text-slate-300 hover:text-white transition-colors" data-title="Trascrizione: Dati geografici" data-content="Buon pomeriggio, sono Leonardo. Il cognome potete leggerlo perché in Sicilia è un problema: lo sbagliamo sempre. All'universo... sia qui nella presentazione. Mi occupo di dati geografici in AgID da circa un anno come, diciamo, &quot;posto fisso&quot;, e da 25 anni come consulente. Non so se qualcuno di voi si occupa di dati territoriali, geografici, spaziali, eccetera. Forse Andrea... e anche Agostino? Ah, ah, perfetto!

                                E quindi, quello che facciamo è essenzialmente valorizzare il patrimonio informativo pubblico, definire dei data model interoperabili e stare dietro ad alcuni processi, tra cui quello di cui parlavamo ancora, relativo ai dati di elevato valore che, lo vedremo, in molti casi hanno l'informazione geografica come elemento sostanziale.

                                Ora, per rispondere anche alla prima domanda: &quot;Cosa fa AgID per i dati di elevato valore?&quot;. È vero, ci sono delle tematiche che sono complicate da gestire, però qualcosa è stata fatta. Per esempio, sui dati catastali, che è storica come... cioè, incredibile! Qualcosa si è fatto anche per l'ANCSU (Anagrafe Nazionale dei Numeri Civici e delle Strade Urbane), anche se conosco le problematiche legate ai numeri, alla georeferenziazione, che noi stiamo cercando, insomma, di portare avanti.

                                Ora, tutto quello che facciamo è legato anche all'evoluzione che poi stanno avendo i sistemi informativi territoriali con i nuovi sistemi di intelligenza artificiale. Molte delle componenti di un sistema informativo cambiano.

                                Per esempio, la parte software, hardware, il personale tecnico, viene in parte, diciamo, dovrebbe essere sostituito da sistemi autonomi. Non c'è più un'organizzazione di riferimento unica, ma più organizzazioni che trattano tematiche differenti fra loro. Ma la cosa più importante sono, appunto, i dati, che diventano sempre di più dinamici. Abbiamo dati satellitari – nel 2026 ci sarà la costellazione IRIDE, speriamo vada tutto bene –, abbiamo dati che sono sottoutilizzati, tipo le auto mobili per il catasto strade che prendono una quantità di informazioni enorme, abbiamo i sensori IoT, abbiamo i gemelli digitali che potrebbero essere integrati, anzi, lo stiamo facendo per l'illuminazione pubblica.

                                Questo, e tutto questo, però, necessita di regole. I dati, prendendo una metafora di Andrea: open data, dati pubblici... metafora con l'acqua per la disponibilità. Nel nostro caso, l'acqua come trasparenza, cioè come modalità per riconoscere le banche dati. E quindi, per questo, la necessità di far riferimento a modelli riconosciuti, anche perché Antonio citava le linee guida per l'adozione dell'intelligenza artificiale. Uno dei presupposti riconosciuti da tutti è che si riducono le tempistiche, i costi e i consumi nell'addestramento se i dati sono strutturati, quindi facilmente riconoscibili.

                                Quello che facciamo, vi dicevo, quindi, è essenzialmente valorizzare ciò che è già disponibile nella Pubblica Amministrazione come dato geospaziale, geografico e territoriale. In realtà sono sinonimi, forse la parte &quot;territoriale&quot; riguarda più i dati di base, cioè quelli topografici, in particolare partendo dai database geotopografici (DBGT), che sono uno strumento, insomma, che a partire dal 2011 sostituiscono la vecchia rappresentazione cartografica della CTR (Carta Tecnica Regionale). In realtà, io ho fatto una ricerca online mentre eravamo in treno: la Regione Sicilia dovrebbe ancora avere una CTR. Rispondo in diretta: è del 2013. È irresponsabile! E noi, ovviamente, siamo passati ai database dal 2011.

                                Abbiamo trovato, e questo è un altro elemento che vale la pena affrontare, molte pubbliche amministrazioni... c'è l'obbligo di produrre database geotopografici dal 2001, però molte pubbliche amministrazioni hanno teso a sintetizzare i contenuti del database topografico, considerandolo molto, non complesso, ma complicato. In realtà, i nuovi sistemi di intelligenza artificiale richiedono sistemi complicati, complessi e ricchi di informazioni. Quindi, deve cambiare il paradigma, diciamo, nell'analisi. E quindi la valorizzazione che noi tendiamo a proporre sui database geotopografici fa riferimento a questo.

                                Altra modalità e altro motivo per cui noi ci occupiamo di dati spaziali è l'interoperabilità. Molte, vedremo qualche esempio, pubbliche amministrazioni centrali e locali, anche regioni, si rivolgono a noi per produrre nuovi data model tematici che hanno nell'informazione geografica un elemento essenziale. In questo caso noi, non avendo competenze di dominio sulle tematiche (illuminazione pubblica, reti di sottoservizio, eccetera), forniamo le nostre competenze per quanto riguarda, appunto, l'interoperabilità, partendo dai database geotopografici. Quindi, supportiamo a definire dei data model tematici che siano interoperabili *by design*, che seguano l'interoperabilità semantica e tecnica, come previsto peraltro dalla normativa di riferimento. Quindi l'obiettivo è poi allargare i contenuti di base dei database topografici con contenuti extra, tematici. Quindi i DBGT diventano DBGT estesi.

                                Altro riferimento di cui ha parlato anche Antonio è i dati di elevato valore (HVD - High-Value Datasets). Almeno tre delle sei serie di dati di elevato valore fanno riferimento direttamente alla componente geografica, ma in realtà, poi lo vedremo, anche i dati meteorologici e statistici normalmente hanno un riferimento geografico, così come i dati relativi alle imprese. Nell'ultima conferenza INSPIRE, c'è stato chiesto se i dati relativi alle imprese potessero essere collocati anche con una georeferenziazione, per creare evidentemente dei sistemi.

                                Dicevo, i database geotopografici, con l'eccezione di cui prima, sono disponibili per tutte le pubbliche amministrazioni e definiscono una nuova – nuova dal 2011, quindi diciamo consolidata – struttura per la rappresentazione dei dati territoriali. Sono strutturati in strati, temi e classi. Questo ci permette di aggiungere in maniera controllata tematiche di volta in volta di interesse. Quindi abbiamo 11 strati, all'interno dei quali troviamo dei temi, all'interno dei quali troviamo delle classi, che sono contenitori di oggetti omogenei rispetto alla rappresentazione geometrica e rispetto al contenuto. Ovviamente, tutto in una relazione di database, quindi con delle relazioni topologiche, geometriche, eccetera. Quindi, insomma, questo... faccio velocemente il passaggio dalla CTR tradizionale al database topografico: è come avere uno shapefile al costo di un raster, anche se ho detto shapefile in questo contesto, scusate.

                                Allora, per parlare sempre del database topografico, l'edificio del CNR in cui siamo ospitati è composto da due classi di oggetti nel database topografico. Il primo, che fa riferimento all'unità volumetrica. L'unità volumetrica è il volume minimo che compone l'edificio e che si differenzia dagli altri volumi per una differenza di altezza, di quota, a una determinata scala, eccetera, eccetera. Quindi la classe &quot;unità volumetrica&quot; ci serve a ricostruire il modello tridimensionale. La classe &quot;edificio&quot; ha sempre una sua componente geometrica, in realtà non ce l'ha perché è un composto di unità volumetriche. Quindi, il poligono &quot;edificio&quot; è composto da unità volumetriche e ha tutta una serie di informazioni, quali la tipologia edilizia, la categoria d'uso, se è sotterraneo o meno, qual è lo stato di conservazione, se è un edificio monumentale, eccetera.

                                In realtà, queste due classi sono solo una piccola parte dell'edificato. Questo è il diagramma di sintesi delle specifiche sui database topografici, che comprende una serie di ulteriori classi. In alcuni casi sono composte da... In altri casi, per scale che vanno dal 1:1000 al 1:2000, ci servono anche a produrre proprio un elemento tridimensionale, anche dal punto di vista degli elementi di copertura. Questo è lo schema generale, sempre semplificato, e questo ci fa capire perché molti abbiano avuto un approccio... In realtà ci sono più di 150 classi, quindi non abbiamo solo l'edificio, ma abbiamo anche i marciapiedi del CNR, l'area di circolazione veicolare, pedonale, ciclabile, eccetera.

                                Beh, vi dicevo, quello che noi facciamo, ci siamo non inventati, ma insomma, era una cosa quasi naturale, abbiamo definito una prassi operativa per aggiungere ulteriori contenuti a quella complicazione. E l'abbiamo fatto supportando diverse pubbliche amministrazioni. Perché abbiamo fatto questo? Perché molti di questi progetti, avendo un riferimento legato al contesto territoriale in cui si inseriscono, non possono prescindere dallo stesso. Per esempio, per fare un progetto illuminotecnico non posso definire un data model a sé stante e non inserirlo nel contesto dell'edificato, della circolazione veicolare e tutto il resto.

                                Questi sono i progetti che abbiamo seguito o abbiamo in corso o delle proposte di cui vedremo nello specifico. E in particolare, sempre su geodati.gov.it, trovate le specifiche, quindi le regole tecniche del SINFI (Sistema Informativo Nazionale Federato delle Infrastrutture), che è stato realizzato in collaborazione con il Mise e il PNRR. Si tratta di due specifiche operative, cioè di due data model che sono stati implementati in piattaforme informative. E poi abbiamo collaborazione anche con delle regioni, in particolare con Regione Umbria stiamo curando la parte urbanistica, sempre integrata nel database topografico. Con Regione Lombardia stiamo partendo con le aree sciabili, è un argomento minimo, diciamo, però è sempre legato, appunto, al territorio. Però dovrebbe essere l'inizio di una nuova storia AgID-Lombardia, che produrrà il database geotopografico integrale. E abbiamo fatto anche delle proposte operative. Forse quella più interessante, anche in questo contesto di Datatalk, può essere quella relativa alla copertura del suolo.

                                > **Pubblico:** Scusate, scusate. La &quot;I&quot; per cosa sta?
                                > **Relatore:** Integrato. Integrato nel senso che integra, cioè che ci hanno chiesto di aggiungere le aree sciabili, per dire, un contenuto tematico che non è previsto nello standard topografico, che noi modelliamo, aggiungiamo.
                                > **Pubblico:** Olimpiadi?
                                > **Relatore:** Questo sarà...
                                > **Pubblico:** E ti posso chiedere cos'è invece il &quot;P scuola&quot;?
                                > **Relatore:** Allora, il PEIL... pensavo di parlarne dopo.
                                > **Pubblico:** Quando vuoi, quando vuoi. Anche dopo.

                                Qual è la modalità con cui aggiungiamo questi contenuti? Ovviamente lo facciamo in maniera controllata. Il framework, in realtà, ripeto, è una prassi operativa, non è ancora stata formalizzata in una linea guida, forse lo faremo. Il primo step riguarda la verifica della presenza del contenuto che voglio modellare nella specifica estesa sui database geotopografici. I database geotopografici hanno dei contenuti obbligatori e un contenitore esteso. Spesso i contenuti che ci chiedono di modellare sono già presenti, quindi basta aggiungere l'obbligatorietà, magari farlo con gli strumenti opportuni. Noi utilizziamo la GML Methodology, UML/GIS, per citarne alcuni.

                                Nel secondo step, cioè nel caso in cui nella specifica estesa il mio contenuto non è presente, verifichiamo i contenuti da aggiungere e verifichiamo se esistono già nella struttura ad albero di cui vi parlavo prima, se esiste già una modalità di integrazione. Vi faccio un esempio: il SINFI, Sistema Informativo Nazionale Federato delle Infrastrutture, è un approfondimento delle reti di sottoservizio, perché proprio tratta quegli argomenti: rete elettrica, acquedotti, rete di riscaldamento, eccetera. Quindi non abbiamo fatto altro che utilizzare lo stesso schema del DBGT aggiungendo dei contenuti: non solo la tipologia di rete elettrica, ma anche le fasi o il tipo di... vabbè, il gestore, eccetera. Per esempio, la portata dei tubi per gli acquedotti non è presente nel database demografico, noi l'abbiamo integrata alla classe acquedotti, aggiungendo appunto i consumi. Per le scuole anche, lo stesso.

                                Per quanto riguarda poi necessità che vanno oltre ciò che è già previsto nel database topografico, aggiungiamo nuovi strati. Allora, vediamo qual è il PEIL di cui si parlava prima. Quello è un progetto generale che riguarda l'efficientamento energetico di più elementi, cioè più oggetti pubblici, diciamo così, contesti pubblici. Noi abbiamo prodotto questa specifica che riguarda l'illuminazione pubblica (PEL-IP). Il progetto è di ENEA, quindi noi forniamo la competenza per la definizione del data model integrato, eccetera, eccetera, e loro forniscono tutte le necessità, i contenuti, ciò di cui hanno bisogno. Questo progetto dovrebbe produrre – almeno all'inizio era questa la cosa che veniva detta –, considerando che ci sono 12 milioni di punti luce pubblici a livello nazionale, l'efficientamento dovrebbe produrre un risparmio di circa 400 milioni di euro l'anno. Solo per un argomento che magari noi non consideriamo neanche. E nello stesso tempo, l'efficientamento consiste, oltre che nella riduzione dei consumi, anche nel miglioramento del servizio, ovviamente. Il PEIL-Scuole riguarda, nell'ambito degli edifici pubblici, le scuole, perché il progetto più generale è &quot;Edifici&quot;, che dovrebbe riguardare poi anche gli ospedali e tutto il resto. In quel caso, noi aggiungiamo alla classe &quot;edificio&quot;, all'oggetto georeferenziato riconosciuto nel database topografico, tutta una serie di informazioni utili all'efficientamento energetico. Quindi si va dagli impianti... è un BIM, diciamo, più leggero.

                                Questo è lo schema che abbiamo prodotto. Quindi ci sono il punto luce come classe effettiva e poi ci sono tutta una serie di contenuti, fra cui quello relativo ai dati dinamici, dovrebbe essere quello più di interesse perché considera i consumi. Esatto.

                                Il SINFI è un altro progetto che abbiamo seguito, anche questo lo trovate su geodati.gov.it. In questo caso, l'obiettivo del progetto è quello di ridurre costi e tempi per la diffusione della banda ultra-larga sul territorio nazionale e quindi ci hanno chiesto di fare un data model che considera tutte le reti di sottoservizi possibili, indicando anche la disponibilità di infrastrutture non utilizzate o parzialmente utilizzate. Ovviamente, la georeferenziazione di questi elementi è fondamentale per quello di cui si parlava prima, cioè per ridurre tempi e costi di realizzazione, considerando che l'infrastruttura è la cosa che costa di più per diffondere, appunto, la banda ultra-larga.

                                Abbiamo fatto poi delle proposte. Una di queste è l'ANCSU, che dovrebbe risolvere il problema della georeferenziazione. La linea guida INSPIRE-Agenzia delle Entrate non la considera obbligatoria, che è un errore di partenza, ma comunque è superato dalla norma europea, in qualche modo superato dalla norma europea con l'atto esecutivo. In questo caso, noi l'avevamo già fatto, cioè integrando nel database per la collaborazione con le regioni.

                                Vi dicevo, un esempio è quello con Regione Umbria, in cui abbiamo cominciato ad aggiungere e considerare anche gli elementi relativi alla pianificazione territoriale, soprattutto urbanistica. Ovviamente, questo è un discorso che va approfondito, perché le leggi urbanistiche sono regionali, quindi i contenuti cambiano da regione a regione.

                                Questo, secondo me, è la cosa più interessante che abbiamo fatto come proposta, che è un data model relativo alla copertura del suolo, anche perché poi c'è un intervento sulla tematica. La mancanza, non ricordo, della... Esatto. E la copertura del suolo è un elemento essenziale da questo punto di vista, tant'è che i report di ISPRA sul consumo di suolo utilizzano proprio questo tipo di dato, perché di fatto discrimina le superfici impermeabili da quelle non impermeabili, da quelle coperte d'acqua e poi ha delle sottocategorie all'interno. Anche in questo caso, noi abbiamo provato a riportare quei contenuti, che sono quelli previsti a livello europeo dalle matrici EAGLE utilizzate da ISPRA, nella struttura dei database topografici. Quindi abbiamo creato un nuovo strato, &quot;Copertura del suolo&quot;, tre temi all'interno dei quali poi ci sono delle classi, ma hanno di fatto tutti i contenuti già richiesti da EAGLE e di conseguenza da ISPRA.

                                Dopodiché, abbiamo fatto un ulteriore passaggio, che secondo me è interessante proprio nel contesto del Datatalk, su come si può lavorare sui dati, cercando di vedere se i contenuti del database topografico fossero sufficienti a produrre i contenuti richiesti dalla copertura del suolo. Cioè, dopo aver creato il data model &quot;Copertura del suolo&quot;, abbiamo provato a individuare, visto che è una copertura topologica, di quali contenuti del DBGT sono composti i contenuti definiti per la copertura del suolo. Quindi abbiamo fatto un piano di mappatura e lo abbiamo anche applicato.

                                In particolare, ovviamente, questa è un'evidenza dell'importanza che ha avere a disposizione un database topografico, magari anche aggiornato, sarebbe meglio. L'abbiamo applicato su un comune dell'Umbria che si era appena dotato di database topografico e questa mappatura... quindi qui vedete la copertura del suolo ISPRA, che già conoscete, disponibile come dato raster. In alcuni casi mi hanno detto che il dato viene prodotto vettoriale e viene generalizzato raster, degenerato, diciamo, raster proprio per ISPRA, è richiesta con il pixel a 10 metri. Abbiamo applicato quella mappatura per il comune di Narni e abbiamo questa soluzione: un dato vettoriale che è stato generato automaticamente facendo la mappatura. Anche perché il database topografico è molto più approfondito, ovviamente, della matrice EAGLE utilizzata per la copertura del suolo, perché ci sono N classi che fanno riferimento al contenuto &quot;copertura del suolo&quot;, &quot;superficie impermeabilizzata&quot;, eccetera, eccetera. Quindi la nostra idea è quella di, anziché fare un database di sintesi, aumentare i contenuti, la complicazione, diciamo, la complessità, proprio per far fronte ai nuovi sistemi di intelligenza artificiale che si nutrono di dati, che non sono petrolio ma acqua, nel senso che devono essere, appunto, trasparenti.

                                Ora vado velocemente, non so... molto sui dati di elevato valore, per farvi capire che proprio i database topografici contengono intrinsecamente dati considerati di elevato valore. E in particolare, le condizioni richieste per i dati di elevato valore sono: il formato leggibile, lo scaricamento in blocco di cui parlava Antonio, l'accesso tramite API, i vocabolari controllati, la data di creazione e aggiornamento regolare e la licenza aperta. Molte di queste condizioni sono già, diciamo, non sono un problema. Altre sono un problema in alcuni casi delle amministrazioni, perché l'aggiornamento regolare non rispetta nessuno standard. E molti di questi, però... il formato leggibile, il vocabolario controllato, sono già intrinsecamente presenti nei database topografici. Quindi, perché non utilizzarli? Considerando anche che la scala richiesta per gli HVD è dal 5.000 andando verso scale più grandi. Quindi non possiamo, se era richiesto il dato sulla rete, il reticolo idrografico, utilizzare il dato dell'IGM al 100.000, perché non è di elevato valore. Dobbiamo considerare un dato che va al 5.000, se disponibile, ovviamente.

                                Perché l'obiettivo dei dati di elevato valore, lo diceva anche Antonio, è riutilizzarli per creare dei vantaggi dal punto di vista ambientale, economico e sociale. Molte delle regioni già hanno questo tipo di dato. Quindi il ragionamento che abbiamo fatto, visto che AgID deve coordinare e supportare le pubbliche amministrazioni per la produzione di dati di elevato valore, è: verifichiamo quei dati di elevato valore che possiamo trovare direttamente nei database. Dopodiché, facciamo un passaggio di conformità rispetto alle *data specification* di INSPIRE, perché di fatto il regolamento richiede la conformità alle *data specification* di INSPIRE, e poi li pubblichiamo. Alcune regioni hanno, inconsapevolmente, già realizzato questo. Per esempio, la Sardegna è una delle regioni... anche se non c'è la conformità ai file, però i contenuti ci sono tutti, effettivamente. Quindi la solita mappatura che facciamo: da una parte gli HVD, considerando le tre serie di interesse e i dataset che rientrano in quelle tre serie, e da questa parte le classi del DBGT in cui troviamo quegli oggetti.

                                Ma facciamo un esempio. In questo caso, l'edificio. HVD &quot;Edifici&quot;, dato di elevato valore appartenente alla serie &quot;Dati geospaziali&quot;. È richiesta la geometria, l'impronta dell'edificio, l'identificatore univoco, il tipo d'uso e il numero dei piani, oltre all'identificativo INSPIRE. Allora, abbiamo provato a vedere se sono già disponibili queste informazioni. In effetti, troviamo tutto partendo dall'unità volumetrica, quindi non dobbiamo fare, aspettare un altro PNRR per fare questo. In più, quello che diceva Antonio, cosa che avremmo dovuto fare già da tempo: questi dati che sono richiesti anche da INSPIRE. In effetti, se noi aggiungiamo... perché questi sono i dati di base considerati come le richieste di base... quelle di &quot;Edifici INSPIRE&quot; richiedono informazioni aggiuntive. E in realtà, anche quelle informazioni aggiuntive sono già presenti nei database topografici.

                                Ultimo esempio è quello delle reti stradali e ferroviarie. Eh, sì, era lo sketch che abbiamo preparato. Anche in questo caso, nello specifico, vengono richieste come dati di elevato valore la conformità rispetto a INSPIRE, e INSPIRE richiede essenzialmente il poligono, la geometria della stazione o di altre infrastrutture che fanno riferimento alla stazione, il link, cioè la tratta ferroviaria, e il nodo, che insieme alla tratta definisce la rete del grafo ferroviario. Abbiamo fatto la verifica, nella quale... scusate, l'accessibilità... questa è la canzone... ci hanno fatto quattro corsi, c'abbiamo anche un esperto, un giovane esperto formatore, un formatore sta diventando rosso...

                                Dicevo, quindi, la stazione in questo caso la troviamo nella classe &quot;Edificio&quot; che ha la categoria d'uso &quot;stazione&quot; o altre tipologie che già sono presenti, quindi copriamo tutto a parte il dato di inizio, il valore della versione, che insomma... La tratta la troviamo, anche in questo caso, non accessibile, nell'elemento ferroviario con tutti i contenuti richiesti, e il nodo ovviamente nella classe &quot;giunzione ferroviaria&quot;. Quindi, anche in questo caso, il dato è già disponibile. Il problema sono gli aggiornamenti, ma già avere a disposizione questo sarebbe... Grazie.

                                [Applauso]">Leggi la trascrizione</button>
                            </div>
                        </div>
                    </div>

                    <!-- Card Intervento 3: Lodi -->
                    <div class="bg-slate-800/50 backdrop-blur-sm border border-blue-800/50 rounded-xl p-6 flex flex-col justify-between shadow-lg hover:shadow-sky-500/20 transition-all duration-300">
                        <div class="flex-grow">
                            <h3 class="text-xl font-semibold text-white mb-2">L’occasione dei dati (aperti) da non farsi sfuggire nell’era dell'IA</h3>
                            <p class="text-slate-400 mb-2">Giorgia Lodi (ISTC-CNR)</p>
                            <p class="text-slate-300 text-sm mb-6">Come sfruttare il potenziale dei dati aperti in un mondo dominato dall'intelligenza artificiale per creare valore e innovazione.</p>
                        </div>
                        <div class="mt-auto pt-4 border-t border-slate-700/60">
                           <div class="flex flex-wrap items-center justify-between gap-3">
                                <div class="flex items-center gap-3">
                                    <a href="https://www.youtube.com/live/I8p-bEY_ago?si=lOao1oUOl4TY539V&t=8657" class="text-center bg-sky-600 hover:bg-sky-700 text-white font-semibold py-2 px-3 rounded-lg transition-colors flex items-center justify-center gap-2">
                                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor"><path d="M2 6a2 2 0 012-2h6a2 2 0 012 2v8a2 2 0 01-2 2H4a2 2 0 01-2-2V6zM14.553 7.106A1 1 0 0014 8v4a1 1 0 00.553.894l2 1A1 1 0 0018 13V7a1 1 0 00-1.447-.894l-2 1z" /></svg>
                                        <span>Video</span>
                                    </a>
                                     <a href="resources/slides/lodi.pdf" class="text-center bg-slate-700 hover:bg-slate-600 text-white font-semibold py-2 px-3 rounded-lg transition-colors flex items-center justify-center gap-2">
                                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M3 17a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zm3.293-7.707a1 1 0 011.414 0L9 10.586V3a1 1 0 112 0v7.586l1.293-1.293a1 1 0 111.414 1.414l-3 3a1 1 0 01-1.414 0l-3-3a1 1 0 010-1.414z" clip-rule="evenodd" /></svg>
                                        <span>Slide</span>
                                    </a>
                                </div>
                                <button class="expand-trigger flex items-center justify-center gap-1.5 text-sm text-sky-400 hover:text-sky-300 font-medium transition-colors py-2 flex-shrink-0">
                                    <span>Approfondisci</span>
                                    <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 transition-transform chevron" viewBox="0 0 20 20" fill="currentColor">
                                        <path fill-rule="evenodd" d="M5.293 7.293a1 1 0 011.414 0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414 0l-4-4a1 1 0 010-1.414z" clip-rule="evenodd" />
                                    </svg>
                                </button>
                            </div>
                            <div class="expandable-content mt-4 space-y-2">
                                <button class="modal-trigger w-full text-left px-3 py-2 text-sm bg-slate-700/50 hover:bg-slate-700 rounded-md text-slate-300 hover:text-white transition-colors" data-title="Riepilogo: Dati aperti e AI" data-content="Giorgia Lodi ha esplorato il potenziale dei dati aperti nell'era dell'intelligenza artificiale per creare valore e innovazione. Ha evidenziato un problema critico degli LLM (Large Language Models): le 'allucinazioni', ovvero risposte che sembrano veritiere ma sono inaccurate. Ha dimostrato questo fenomeno confrontando le risposte di ChatGPT sui beni culturali italiani con i dati precisi e aggiornati del knowledge graph aperto del Ministero della Cultura (ArCo), rivelando significative discrepanze. Questo sottolinea la necessità di integrare gli LLM con banche dati esterne e strutturate per migliorare l'affidabilità delle risposte.

                                Lodi ha poi analizzato i limiti degli LLM: sono 'scatole nere' con scarsa spiegabilità dei risultati, ignorano i vincoli sui dati e non possono esistere senza dati. Ha sostenuto che i dati aperti rappresentano una soluzione ideale per mitigare questi problemi. L'apertura dei dati, infatti, consente la mitigazione delle allucinazioni (ad esempio, tramite tecniche come la Retrieval Augmented Generation), aumenta la trasparenza (richiesta dall'AI Act per i sistemi ad alto rischio), e fornisce una base dati fondamentale per l'addestramento dell'IA, mettendo tutti sullo stesso piano di riutilizzo.

                                L'intervento ha enfatizzato la necessità di una 'data quality by design', citando i principi FAIR (Findable, Accessible, Interoperable, Reusable) e le caratteristiche di qualità degli standard ISO. Ha criticato la scarsa attenzione alla qualità dei dati nelle pubbliche amministrazioni italiane, come rilevato da una recente indagine AgID. Lodi ha concluso che la PA ha un ruolo cruciale nella costruzione di una solida infrastruttura dati e che è indispensabile un cambiamento culturale nel modo di gestire i dati, superando l'approccio documentale per adottare un modello 'data driven', in cui i dati aperti di alta qualità diventano la spina dorsale per l'innovazione guidata dall'IA.">Leggi il riepilogo</button>
                                <button class="modal-trigger w-full text-left px-3 py-2 text-sm bg-slate-700/50 hover:bg-slate-700 rounded-md text-slate-300 hover:text-white transition-colors" data-title="Trascrizione: Dati aperti e AI" data-content="Grazie, grazie per l'invito. Dicevo prima che ho scelto di venire qua in presenza, quindi... insomma, perché avevo fatto questa cosa anche a Trieste e mi dispiace di avervi fatto cambiare anche il programma.

                                Eh, io oggi volevo cercare di, eh, qui, parlare di questa occasione dei dati aperti (l'ho messo tra parentesi perché, in realtà, in generale, io adesso mi occupo non solo dei dati aperti, ma di dati a 360°) e, però, secondo me, a maggior ragione, i dati aperti nell'era dell'intelligenza artificiale... e cerchiamo di capire perché, no?

                                E di solito, quando faccio questa presentazione, adesso dove ci si mette l'intelligenza artificiale, perché sennò non si è nessuno nella vita ormai, ci metto sempre degli esempi con ChatGPT per far capire. Ho scelto ChatGPT, avrei potuto scegliere qualunque altro modello di intelligenza artificiale generativa di ultima generazione, per farvi capire un po' il perché voglio arrivare a parare in un certo punto.

                                Allora, ho chiesto a ChatGPT... ho fatto due, in realtà, ho fatto due domande, no? Ho voluto cercare di capire se ChatGPT conosce qualcosa sui nostri beni culturali italiani. Gliel'ho fatto in inglese perché è meglio, insomma, no? Perché poi, lo sapete, che con le lingue anche questo è un altro problema di questi LLM. Ehm, e gli ho chiesto il numero di siti, diciamo, che sono liberamente accessibili nella regione Lazio. Una risposta piuttosto ampia e argomentata, diciamo. E vabbè, lì poi ti ritornano... e dice (ve l'ho evidenziato in rosso) che ci sono circa dai 200 ai 250 siti, eh, quindi luoghi della cultura nella regione Lazio, che offrono un accesso libero sia permanentemente oppure in certi giorni specifici della settimana, per esempio, come può essere la prima domenica del mese. Ok.

                                Questa risposta è interessante, nel senso che per un utente normale, ok, è una risposta molto buona, eh, però molto probabilmente... Poi gli ho fatto un'altra domanda. Gli ho detto: &quot;Quanti posti a Milano sono classificati come gallerie d'arte?&quot;. E, eh, e lui mi ha detto che, secondo i dati recenti, insomma, disponibili, Milano ha circa dai 90 ai 100 posti classificati come galleria d'arte. Ok. Quindi anche questa risposta, se vedete, è abbastanza simile, no?

                                E quindi, ok, però mi sono detta: &quot;Vabbè, ma noi abbiamo i Linked Open Data dei beni culturali italiani, no? E quindi, perché non chiedere al Ministero della Cultura, che li pubblica e tiene speranzosamente aggiornati, le stesse identiche domande, per vedere se il risultato è uguale?&quot;. Ehm, e quindi l'ho chiesto ad ArCo. ArCo è la rete, no? È il knowledge graph, il grafo della conoscenza aperto dei beni italiani. Fatta la prima... Naturalmente, per chiederlo al knowledge graph, devo fare delle query SPARQL. Ok. Io un po' di SPARQL lo conosco, magari non proprio tutti tutti, però ci ho provato, no? E ho fatto una mia query SPARQL e alla prima domanda mi ha risposto: 432. Mi ricordo che ChatGPT aveva detto tra i 200 e i 250, quindi in realtà sono il doppio, ok, eh, più o meno. E nella seconda domanda, eh, lui aveva risposto tra i 90 e i 100. In realtà sono 58. Mh. Eh, allora, qui c'è qualcosa che non va. Ok.

                                Eh, questo per dirvi che: attenzione a usare quei software in un certo modo. Vi dico anche perché. Un po' di tempo fa ChatGPT non aveva, adesso ce l'ha, la possibilità di abilitare la ricerca sul web. Non l'avevo abilitata. Ho usato l'LLM così come nasceva, ok? Quella funzionalità di andare a cercare un sito web è nata successivamente e non è una cosa intrinseca dell'LLM, è aggiuntiva, che lui usa per aiutarsi a migliorare le sue risposte. La cosa che sicuramente emerge è che queste risposte a volte sono sbagliate, semplicemente cosiddette sbagliate.

                                Allora, cosa abbiamo cercato di fare? Anche nelle mie tante vesti e cappelli che ho, tra socia di una spin-off del CNR, laboratorio di ricerca, ho detto: &quot;Vabbè, ma scusatemi, se io ho questi dati aperti così puntuali, precisi, aggiornati e quant'altro, perché non li uso per fare in modo che ChatGPT mi risponda in maniera puntuale?&quot;. Ok? Quindi cerco di mettere in piedi delle tecniche affinché gli LLM usino queste banche dati strutturate per poter migliorare le loro risposte. E infatti, così abbiamo fatto. Abbiamo cercato di mettere insieme ChatGPT e i dati aperti e, se guardate la risposta, adesso la risposta diventa corretta, cioè lui ti risponde &quot;432&quot;, esattamente com'è la query SPARQL. E la stessa cosa capita con l'altra: dice &quot;58&quot;, esattamente com'è il risultato della query.

                                Ora, che cosa ho fatto qua? In realtà, semplicemente, ho detto agli LLM, e quindi all'intelligenza artificiale generativa: &quot;Usa delle banche dati esterne per migliorare i tuoi risultati&quot;. Questo perché? Perché, eh, lo sappiamo, o se non lo sappiamo adesso dobbiamo iniziare a capirlo per bene fino in fondo, che, ehm, questi software di intelligenza artificiale producono quelle che vengono chiamate &quot;allucinazioni&quot;.

                                Allora, è un termine, lo dicevo anche recentemente sui social network, che non mi piace, &quot;allucinazione&quot;, però tecnicamente ormai è il termine che si usa per indicare il fatto che è uno dei grossissimi problemi: producono risposte a volte che non sono vere, ma sembrano verosimili. Cioè, una persona che non sa niente di quello che vi ho raccontato finora si fida. Il &quot;200, tra i 200 e 250&quot;... poi, tra l'altro, &quot;tra i 200 e 250&quot;, quindi c'è un range, no? E quindi è portato a fare questa cosa. Io mi ricordo, qualche... boh, forse l'anno scorso, c'era un colloquio tra due ragazzi in autobus e che dicevano... lei diceva a lui: &quot;Ma perché non l'hai chiesto a ChatGPT l'orario in cui passa l'autobus?&quot;. E io... e volevi rispondere o no? E io, a parte che volevo rispondere... ma vabbè. Ma il discorso, però... assurdo! Ma è stato, secondo me, il fatto di aver messo a disposizione, lasciatemi dire, alle masse, dei software del genere, che da un lato ha avuto dei, secondo me, degli aspetti stra-positivi, dall'altro lato ci sono degli aspetti negativi e bisogna stare attenti.

                                Di recente c'è stato un post, ha pubblicato un articolo che si intitolava: &quot;Perché spesso ChatGPT si inventa le cose?&quot;, no? Per parlare appunto di queste allucinazioni. E c'è un ricercatore del Politecnico di Milano, Stefano Zanero, non so se lo conoscete, molto famoso nell'ambito della cybersecurity italiana, che diceva: &quot;Guardate che loro non è che si inventano spesso le cose, lo fanno sempre&quot;. Cioè, è il loro *modus operandi*, perché loro semplicemente si basano su dei modelli matematici, dietro, statistici, e vanno a vedere qual è la probabilità che una parola sia seguita da un'altra parola. Come fanno a fare questo? Hanno un corpus su cui sono stati addestrati, dei dati in quel caso non strutturati, perché la maggior parte è testo, eh, dove deducono, attraverso tutta una serie di, eh, pesi che attribuiscono alle varie parole all'interno di un certo discorso. Ed è impressionante, a volte, seriamente impressionante. Così come è impressionante il fatto che, nelle domande di prima, se abilitate la ricerca sul web, lui andava a cercare nel sito giusto, ok? Non un sito a caso, ma proprio nel sito giusto per andare a trovare la risposta. Quindi ci sono delle cose abbastanza *impressive*, ok?

                                Però bisogna stare attenti, perché appunto, questo fenomeno è lì, c'è, dobbiamo tenerlo in considerazione. Dobbiamo essere consapevoli, quando usiamo questi software, che questo fenomeno c'è e dobbiamo lavorare per mitigare.

                                L'altro punto fondamentale, secondo me, di questi software è che sono delle &quot;scatole nere&quot;, no? Nel senso che, in realtà, nessuno ancora è in grado di spiegare che cosa avviene là dentro. Nel senso che, eh, vi dicevo prima, ci sono dei modelli statistici alla base di questi software, ok? È tutto basato su pesi, eh, che vengono attribuiti. Se questi pesi, tra l'altro, non sono aperti, quindi non sono open source, per esempio, è difficile riuscire a capire come lui si costruisce il suo modello. Perché fa questo? In realtà, in base al dato di input: quindi tanto il dato di input cambia, tanto anche il modello dentro si può adattare e crea questo modello per produrre un risultato. Ma noi non lo sappiamo qual è, effettivamente, eh, come ci arriva a produrre quel risultato. E infatti, spesso si dice che non si riesce a spiegare il perché mi ritorna quel risultato lì. Pensate: se non so nemmeno qual è il dato di input, come posso arrivare a capire qual è il suo risultato finale? No?

                                E altra caratteristica, e questo è il grande tema di cui si accennava poco fa, è che ignorano completamente (questo è un po' l'uso anche che ne viene fatto da coloro che hanno sviluppato questi software) qualunque tipo di vincolo che può esistere sul dato, che sia esso strutturato o non strutturato. Io dico sempre che fanno un po' &quot;di cojo cojo&quot;, nel senso che fanno scraping sul web di qualunque pagina, ok? E attenzione, usano molto (e qua c'è uno studio a livello UK) i siti web delle pubbliche amministrazioni, fanno scraping di quelle informazioni. Quindi ragionate anche su questo: se il vostro sito web della Pubblica Amministrazione non è aggiornato, pensate che cosa può venire fuori, visto che già non producono risposte veritiere per definizione. Ok? E, eh, ignorano tutto questo tema. Ed è un tema enorme, seriamente enorme, che secondo me ancora non è stato affrontato. Tra l'altro, il risultato che loro producono, di chi è la proprietà intellettuale? Chi l'ha prodotto? La macchina stessa o sono tutti gli input che lui mette insieme che producono questa cosa? Cioè, ci sono dei ragionamenti lì fuori, nell'uso di questi software, che noi dovremmo iniziare a fare e che, secondo me (adesso vi arrivo a sperare, spero di convincervi), gli Open Data possono essere un buon veicolo per cercare di tamponare questa situazione.

                                Infine, non esistono se non ci sono dati, ok? Cioè, non esiste intelligenza artificiale se non ci sono dati, non avrebbe nessun tipo di senso. E il risultato dipende fortemente dal dato di input, per come vi dicevo prima, per come sono tecnicamente fatti. Quindi che cosa vuol dire questo? Che se quel dato non è completo, per esempio, il risultato non potrà mai essere completo. Mi ricordo che c'era un esempio, e spesso lo ricordo, dell'uso dell'intelligenza artificiale nell'ambito della giustizia americana, che poi è stato completamente vietato a un certo punto perché era eclatante. I dati di input erano, la maggior parte delle volte, sentenze contro persone di colore e quindi il risultato era sempre che si andava a imputare una persona di colore, magari quando forse il reato era stato commesso, in quel caso, da una persona invece, eh, bianca, diciamo così. Ecco, questo per dirvi che attenzione, perché se poi li usiamo per prendere delle decisioni che hanno un impatto sulla nostra vita, diventa abbastanza, eh, problematico.

                                E allora io dico, e qui riporto una cosa che dice, in realtà, non io, eh, perché ormai è già stato detto tutto dall'Open Data Institute: secondo me bisogna ripartire da questi principi. Questo è stato pubblicato proprio non molto tempo fa da loro, come una sorta di manifesto d'intenti, diciamo così. E ci sono due principi che mi hanno colpito nello specifico. Il primo dice che ci vuole, nell'era dell'intelligenza artificiale, una *strong data infrastructure*, ok? Perché dobbiamo costruire un qualcosa che sia, eh, un ecosistema di dati di fiducia, in un qualche modo. E per costruirla, dobbiamo tenere in considerazione tutto lo spettro dei dati, ok? Tra l'altro, questa figura che vedete qua è vecchissima. Io, tra l'altro, insegno anche all'università ai miei studenti. È una figura, secondo me, che veramente vi dice un po' tutto lo spettro dei dati, guardando anche diverse dimensioni: se sono piccoli, medi, grandi; se sono personali, commerciali o del governo (del settore pubblico); se sono chiusi, se sono condivisi e se sono completamente aperti, no?

                                Quindi, per gestire tutto questo, ok, noi abbiamo bisogno di, eh, nell'era dell'intelligenza artificiale, avere delle pratiche che mi consentono di gestire in maniera forte. E dicono anche che la miglior possibile *foundation*, quindi il pilastro di tutto questo, è il dato aperto. E io perché dico di ripartire da qua e da questa cosa? Perché riprendiamo i problemi e i limiti che abbiamo visto prima, no? E come i dati aperti ci possono aiutare?

                                Allora, le allucinazioni: vi ho portato un esempio pratico dove le allucinazioni erano lì, però ho usato i dati aperti e ho migliorato i risultati. Quindi, eh, in questo caso, da un punto di vista tecnico, ho usato una tecnica che si chiama RAG, che sta per *Retrieval-Augmented Generation*. Sostanzialmente, gli do una base di conoscenza, ok? Posso farlo anche con una banca dati che è completamente chiusa, eh, non è necessariamente con dati aperti. Però, eh, ho usato questa tecnica per cercare di mitigare e ridurre l'effetto delle allucinazioni. E i dati aperti sono uno strumento, secondo me, micidiale da questo punto di vista, perché chiunque li può guardare, i dati aperti, quindi chiunque può anche riconoscere la qualità di quei dati, ok? Nel momento in cui noi ci apriamo e ci offriamo all'intelligenza collettiva, ci apriamo anche alla possibilità di poter migliorare questi dati, e questo non può che essere uno strumento fondamentale per cercare di mitigare un fenomeno come quello dell'allucinazione.

                                Eh, abbiamo detto che sono delle &quot;scatole nere&quot;, no? Quindi, mi raccomando, una cosa importante: non fanno ragionamenti, non hanno un'idea di concetto, di contesto di questo tipo. Io a volte sento: &quot;Ah, ma arrivano ad avere emozioni!&quot;. No. No, assolutamente no. È tutta roba iper-mega-statistica, ok? Con modelli matematici molto spinti dietro, ma non hanno queste capacità, ok? Perché qui dico che, eh, i dati aperti, nel caso delle &quot;scatole nere&quot;, potrebbero aiutare? Intanto, perché se, ehm, io potrei, per esempio, se uso dei dati aperti in input, spiegare perché arrivo a un certo risultato. Qualche anno fa (perché nel mondo della ricerca siamo sempre un pochino più avanti, ammettiamolo) c'è stata una bellissima presentazione a un forum, insomma, della comunità del Semantic Web (tra parentesi), che faceva vedere come, dato un knowledge graph, ehm, tra l'altro con delle immagini, ok, questi software potevano essere migliorati e si poteva spiegare come loro arrivavano a un certo risultato e come potevano arrivare al risultato corretto, usando appunto strumenti di dati strutturati in un certo modo, con una certa semantica e che spiegano anche il significato dei dati.

                                Quindi, l'explainability di questi software, che è un altro tema di ricerca molto ampio, potrebbe essere assolutamente indirizzata con la disponibilità di dati aperti riutilizzabili da chiunque. Perché i dati aperti possono essere anche proprio utilizzati per l'allenamento di questi software, anche per il famoso principio di trasparenza che ci impone l'AI Act (che sapete, no?, che è questa norma europea) che dice che i software di intelligenza artificiale devono essere classificati in base a un certo rischio e, se sono ad alto rischio perché hanno un impatto e un risultato sugli esseri umani, devono rispettare condizioni di trasparenza. Quindi, per esempio, dovreste dire quali sono i dataset su cui voi allenate il software, ok? Allora, se sono dati aperti, tanto più è agevolato questo compito.

                                Eh, ignorano i vincoli sui dati. Ma noi abbiamo i dati aperti! I dati aperti non hanno vincoli, o se li hanno, ce li hanno pochi, no? E quindi forse riusciamo anche a mettere a posto un pochino questo aspetto legale enorme che loro hanno, no? Quindi, perché no? Perché non sfruttare questo fatto di usare i dati aperti in questo senso? Poi mi rendo conto che non è che tutti i dati sono esclusivamente dati aperti, perché bisogna bilanciare: ci sono anche dati personali che devono essere trattati in un certo modo. Però c'è una grande fetta di dati che può essere dato aperto e può essere utilizzata.

                                Infine, abbiamo detto che senza dati non esistono. I dati aperti sono per definizione dei dati che dovrebbero, dovrebbero, dovrebbero avere un formato aperto e quindi *machine-readable*. Dovrebbero. Ok? E quindi, a maggior ragione, vuol dire accessibili, disponibili, no? E quindi ci aiutano tantissimo per poter, eh, usare questi software. Lasciatemi dire anche, visto che ho una piccola società, una spin-off del CNR: se ci fossero più dati aperti di qualità non sarebbe male anche per noi, perché sarebbe un vantaggio anche di competizione. Perché una società in miniatura come la nostra come fa a competere con OpenAI, Microsoft, che c'ha tutto il mondo, o Google? Ok? È chiaro che non riesce a competere. Se invece abbiamo i dati aperti, che mettono tutti, nel riutilizzo, allo stesso piano, è chiaro che pure io posso fare qualcosa.

                                Quindi, ritornando sempre all'Open Data Institute, i dati aperti potrebbero diventare veramente l'infrastruttura ideale per l'allenamento di questi software di intelligenza artificiale. E loro, guardate che cosa dicono qua (tra parentesi): dicono che un dato che è pronto per l'addestramento all'intelligenza artificiale significa, in pratica, eh, applicare diverse pratiche, tra cui i principi FAIR, ok? *Findable, Accessible, Interoperable, Reusable*. E non solo per i dati della ricerca, ma per tutti i dati. Ok? Linked Data, ok? E qui, vabbè, torna un po' la mia battaglia nazionale, insomma, diciamo così. Perché? Perché i Linked Data sono dati collegati ad altri dati per definizione, con una semantica, e quindi possono avere tutti quei vantaggi che vi dicevo precedentemente. E dicono anche che nel creare dati aperti bisogna avere in mente queste potenzialità dei software di intelligenza artificiale. Cioè, dicono: &quot;Guardate che il governo dovrebbe garantire di pubblicare dei dati di alta qualità, secondo tutte quelle buone pratiche, proprio per evitare che magari questi software vadano a cercare dati in sorgenti secondarie, magari non autoritative come nel caso del governo, proprio per dare un risultato migliore&quot;. E soprattutto quando si parla di intelligenza artificiale applicata, per esempio, ai servizi pubblici, ok? Senza affidarsi a Tizio che scrive nel suo blog che quel servizio pubblico è fatto in questo modo. Ok?

                                Eh, vi ho lasciato i link. Ora, tutto questo è fighissimo, no? Cioè, abbiamo i dati aperti, risolto tutto, abbiamo risolto. E, eh, c'è sempre un &quot;ma&quot; nella vita, sempre. E il &quot;ma&quot; è che funziona così: *garbage in, garbage out*. Ok? Se voi non curate tanto il dato, quindi il dato non è di qualità (anche, io dico, sfruttando la conoscenza di dominio degli esperti, *human-in-the-loop*, non può essere sempre e solo tutto automatico), se non è quello, è chiaro che il risultato non sarà di alta qualità. E gli impatti possono essere poco affidabili, cioè il risultato è poco affidabile, quindi non me ne faccio granché e rischio di magari spendere anche un fottio di soldi per poi non avere una soluzione efficace.

                                E infatti, e qui veniamo a quello che dice AgID, che ho trovato proprio qualche giorno fa e ho detto: &quot;Questo lo devo inserire nelle mie presentazioni perché è fondamentale&quot;. Ha pubblicato la prima (visto che ci sono anche rappresentanti, viene a fagiolo, insomma) indagine sull'uso dell'IA nelle Pubbliche Amministrazioni centrali. Centrali, ok? Non quelle locali. E ha analizzato tipo 120 progetti, se non ricordo male. Sono tantissimi, eh, 120 progetti sono veramente tanti. In realtà, più di machine learning che di intelligenza artificiale generativa, dicono. E mi ha colpito quella cosa che ho scritto in rosso, ehm, che ho evidenziato in rosso. Intanto, il fatto che fanno allenamento &quot;loro&quot;, ok? Però forse ci sta, perché nel machine learning, se è supervisionato, devi fare tu i dataset, devi allenare, quindi puoi usare delle tue banche dati interne, quindi ci può stare. Però dice: &quot;Si rileva scarsa attenzione alla qualità dei dati, con possibili impatti negativi sull'affidabilità&quot;. Quindi la scarsa attenzione... cioè, non è tanto &quot;è successo qualcosa&quot;, cioè, proprio è come dire: &quot;Non mi interessa&quot;. Ok. E questo, intanto, tanto di cappello ad AgID che ha scritto una cosa del genere sul sito web ufficiale dell'istituzione. E poi è il vero dramma, diciamo, di tutta la situazione che io vedo in questo momento su questo.

                                Quindi noi abbiamo bisogno, io dico e ho detto, non più di *open data by design*, non più *privacy by design*, ma *data quality by design*, che vuol dire tutte e due le cose insieme, ok? Perché la *data quality* è una cosa che con Antonio lo sappiamo bene, insomma, abbiamo scritto delle linee guida in passato dove abbiamo parlato di qualità del dato, abbiamo parlato degli standard ISO di qualità del dato, il 25012. Ci sono 15 caratteristiche di qualità del dato, tra cui la *timeliness*, ok? Il tempo giusto. Io ho visto, e questo lo dico ad Antonio: &quot;Ti prego, togliete, togliete da dati.gov.it quei cavolo di dataset, anche di ministeri importanti, centrali, aggiornati al 2021 con una frequenza di aggiornamento annuale!&quot;. È veramente da mettersi i brividi. Ok? Lì c'è qualcosa che non funziona. Ok. Oppure, non lo so, se si sistema il processo... non lo so che cosa sia successo, ma non può... è stato il Covid? È il Covid nel 2021, effettivamente è il Covid. Però io dico una cosa: qui noi dobbiamo avere per i dati i principi FAIR. Sempre. Generali, sempre. Almeno. Senza quelli non si va da nessuna parte, è il minimo. Dobbiamo iniziare da quello, ed è già difficile raggiungerli, ok? È già quello.

                                Le caratteristiche di qualità dei dati, che si possono comporre con i principi FAIR... l'abbiamo anche fatto con &quot;Dati, come vorrei&quot;, ehm, insieme. E secondo me dobbiamo agire in maniera diversa fin dalle prime fasi del processo. Intanto, bisogna porsi le giuste domande sui dati a cui si vuole rispondere, cose che non facciamo quasi mai, e anche quello determina un po' come li raccolgo. Due: dobbiamo raccoglierli con i principi di qualità in testa. Io a volte sento: &quot;Ah, ma sai, nel form la data era obbligatoria, però poi nel dato non c'è&quot;. E quindi come fa a essere obbligatoria? Se nel form è obbligatoria, mi devi bloccare se non ce la metto. Cioè, già lì tu devi controllare la qualità del dato. Perché ho come l'impressione, in moltissimi sistemi che adesso anche sto vedendo, che si fa sempre dopo, a valle. Cioè, si consente la &quot;monnezza&quot; iniziale, di qualunque tipo, e poi intanto dopo uno dice: &quot;Vabbè, dopo nel processo sistemo&quot;. Ma quel &quot;dopo nel processo sistemo&quot; è difficilissimo, è molto più dispendioso. Fatelo da prima! Controllate i dati da prima in termini di qualità.

                                Quindi, concludendo, perché è tardi e vi voglio anche lasciare andare... (Eh no, come no? Interventi... è il nostro portavoce, quindi vai!). La prima cosa è che, secondo me, la Pubblica Amministrazione, con appunto questa idea anche dell'apertura dei dati, può avere un ruolo cruciale nella costruzione di quelle infrastrutture che dice l'ODI, non Lodi, l'apostrofo ODI (Open Data Institute), ok? Quell'infrastruttura del *data-as-a-service*, cioè 'sta roba di: &quot;Abbandoniamo il *document-based*, no? Adottiamo la PA *data-driven*!&quot;. Finalmente. Questo lo dico... lo dicevamo già, tra l'altro sempre con Antonio, secoli fa, ma ora più che mai questo deve valere, ok?

                                E in particolare, proprio con gli open data. E vi dirò anche di più: gli open data di adesso non vanno bene. Mi dispiace, questo è un messaggio netto da dare. Gli open data che noi abbiamo adesso in Italia, tranne forse qualche eccezione, non vanno bene per fare le cose che abbiamo visto prima. Bisogna alzare l'asticella, perché, appunto, c'è scarsa attenzione alla qualità del dato. Dobbiamo fare molto, ma molto, ma molto di più. Cioè, dobbiamo andare oltre il dire: &quot;Pubblico su Amministrazione Trasparente, uno scarica l'open data e lì finisce&quot;. Ok?

                                E un'altra cosa molto dura che vi dico è che, secondo me, bisogna mettersi nella testa (e qui è un concetto di cultura del dato) che il modo con cui noi lavoriamo con i dati non è più quello anche solo di 5 anni fa. Non voglio andare tanto indietro nel tempo, ma anche solo di 5 anni fa: non è più quello. Il mondo è completamente cambiato da un punto di vista tecnologico, ci sono delle sfide enormi che avete visto e non possiamo più gestire il dato come abbiamo sempre gestito finora. Lo so che cambiare è difficile, però dobbiamo considerare di rivedere completamente i processi. E questa è un'altra cosa che dico: l'intelligenza artificiale in Italia potrebbe fallire se noi facciamo l'errore che abbiamo fatto con altre soluzioni, anche ai tempi del Covid, di buttare questo elemento dentro dei processi che sono quelli che noi abbiamo da non so quanto tempo a questa parte. Non saranno mai efficaci, non produrranno mai dei risultati efficaci. Bisogna ripensare a tutta la filiera e bisogna mettersi lì e farlo. Ci vuole tempo? Sì. Ci vuole sforzo? Sì. Ma lo dobbiamo fare, se vogliamo avere dei risultati concreti e se crediamo veramente nel valore aggiunto dei dati e dei dati aperti, che sono quelli, come c'è scritto qua, che in un qualche modo possono essere usati per l'allenamento, mettendo tutti allo stesso piano perché sono più democratici di altri dati. Ok.

                                Io ho concluso, vi ringrazio.">Leggi la trascrizione</button>
                            </div>
                        </div>
                    </div>

                    <!-- Card Intervento 4: Angemi -->
                    <div class="bg-slate-800/50 backdrop-blur-sm border border-blue-800/50 rounded-xl p-6 flex flex-col justify-between shadow-lg hover:shadow-sky-500/20 transition-all duration-300">
                        <div class="flex-grow">
                            <h3 class="text-xl font-semibold text-white mb-2">Scarsità d'acqua e abbondanza di dati: il contributo degli LLM</h3>
                            <p class="text-slate-400 mb-2">Dennis Angemi (Open Data Sicilia)</p>
                            <p class="text-slate-300 text-sm mb-6">Un caso d'uso su come i LLM possono estrarre dati aperti da documenti non strutturati relativi all'emergenza idrica in Sicilia.</p>
                        </div>
                        <div class="mt-auto pt-4 border-t border-slate-700/60">
                           <div class="flex flex-wrap items-center justify-between gap-3">
                                <div class="flex items-center gap-3">
                                    <a href="https://www.youtube.com/live/I8p-bEY_ago?si=YVVeXT_BeU8Xw6iN&t=12133" class="text-center bg-sky-600 hover:bg-sky-700 text-white font-semibold py-2 px-3 rounded-lg transition-colors flex items-center justify-center gap-2">
                                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor"><path d="M2 6a2 2 0 012-2h6a2 2 0 012 2v8a2 2 0 01-2 2H4a2 2 0 01-2-2V6zM14.553 7.106A1 1 0 0014 8v4a1 1 0 00.553.894l2 1A1 1 0 0018 13V7a1 1 0 00-1.447-.894l-2 1z" /></svg>
                                        <span>Video</span>
                                    </a>
                                     <a href="resources/slides/angemi.pdf" class="text-center bg-slate-700 hover:bg-slate-600 text-white font-semibold py-2 px-3 rounded-lg transition-colors flex items-center justify-center gap-2">
                                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M3 17a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zm3.293-7.707a1 1 0 011.414 0L9 10.586V3a1 1 0 112 0v7.586l1.293-1.293a1 1 0 111.414 1.414l-3 3a1 1 0 01-1.414 0l-3-3a1 1 0 010-1.414z" clip-rule="evenodd" /></svg>
                                        <span>Slide</span>
                                    </a>
                                </div>
                                <button class="expand-trigger flex items-center justify-center gap-1.5 text-sm text-sky-400 hover:text-sky-300 font-medium transition-colors py-2 flex-shrink-0">
                                    <span>Approfondisci</span>
                                    <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 transition-transform chevron" viewBox="0 0 20 20" fill="currentColor">
                                        <path fill-rule="evenodd" d="M5.293 7.293a1 1 0 011.414 0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414 0l-4-4a1 1 0 010-1.414z" clip-rule="evenodd" />
                                    </svg>
                                </button>
                            </div>
                            <div class="expandable-content mt-4 space-y-2">
                                <button class="modal-trigger w-full text-left px-3 py-2 text-sm bg-slate-700/50 hover:bg-slate-700 rounded-md text-slate-300 hover:text-white transition-colors" data-title="Riepilogo: Scarsità d'acqua" data-content="Dennis Angemi ha presentato un caso d'uso sull'applicazione degli LLM per estrarre dati aperti da documenti non strutturati, con un focus sull'emergenza idrica in Sicilia. Ha descritto la problematica della scarsità d'acqua nella regione e la difficoltà di accedere a dati aggiornati e strutturati sugli invasi idrici, spesso pubblicati in formato PDF, il livello più basso nella scala di qualità dei dati aperti. Ha dimostrato come i più noti LLM (ChatGPT, Gemini) falliscano nel fornire informazioni accurate e aggiornate da fonti non strutturate.

                                Per affrontare questa sfida, Angemi ha illustrato un approccio che utilizza un LLM per automatizzare l'estrazione dei dati dai PDF. Il processo prevede l'identificazione dei documenti, seguita da tre chiamate all'API del modello: due estrazioni guidate (una con anagrafica dati per un matching accurato, una senza) e una fase di sanificazione e validazione incrociata degli output per ridurre al minimo le 'allucinazioni'. Una volta estratti, i dati vengono aggiornati quotidianamente in un repository GitHub, rendendoli disponibili in formati strutturati come CSV e con metadati frictionless.

                                Angemi ha evidenziato il riutilizzo di questi dati da parte di enti come il Centro Meteorologico Siciliano. Tuttavia, il dibattito scaturito dall'intervento ha criticato la resistenza delle pubbliche amministrazioni, in particolare la Regione Siciliana, nel pubblicare dati in formati aperti, nonostante gli obblighi di legge e la dimostrata capacità di organizzazioni esterne come Open Data Sicilia di estrarli e renderli fruibili. Ha denunciato la mancanza di attenzione all'importanza dei dati e alla trasparenza, sottolineando come questa inerzia impedisca di cogliere il valore potenziale dei dati per la governance e i servizi pubblici.">Leggi il riepilogo</button>
                                <button class="modal-trigger w-full text-left px-3 py-2 text-sm bg-slate-700/50 hover:bg-slate-700 rounded-md text-slate-300 hover:text-white transition-colors" data-title="Trascrizione: Scarsità d'acqua" data-content="
                                Vi dicevo, era giugno 2024. A casa mia, il getto dell'acqua, la portata, comincia a diminuire. Dopo qualche minuto, il getto dell'acqua diventa questo. Dopo qualche minuto ancora, se la portata stava diminuendo, è diventata questa. E il problema, in qualche modo l'avete intuito, è la siccità.

                                Eh, comincia... allora, la Regione ricomincia a chiedere lo stato di emergenza nazionale. È avvenuto nel 2024, ma è avvenuto anche un mese scarso fa. Si è riproposto identico lo stesso scenario. La Regione, quindi, avanza formalmente la richiesta di dichiarazione per lo stato di emergenza, ed è il 3 aprile del 2024. Quindi, ancora prima della soluzione, cominciano queste riduzioni idriche.

                                Eh, peccato che in alcuni comuni queste riduzioni sono state così importanti che hanno portato veramente a dinamiche anche sociologiche importanti. Ad esempio, in un comune in provincia di Enna, che è la provincia dalla quale vengo io, l'erogazione idrica avveniva ogni 5 giorni. Immaginate voi: una famiglia di cinque persone con l'acqua a casa ogni 5 giorni. È il comune di... l'unico? No. Il Comune di Gagliano, per dire. Un comune sempre in provincia, uguale: ogni 5 giorni. Ma, in realtà, questa cosa si è riproposta in tantissimi comuni.

                                E perché vi dicevo che ci sono delle implicazioni sociologiche importanti? Perché i sindaci cominciano ad occupare le sedi, perché queste riduzioni idriche erano legate alla gestione degli invasi. E quindi, una delle dighe importanti per questo territorio, la diga chiamata Ancipa, comincia ad essere occupata dai sindaci. Ma non funziona. Anzi, addirittura alcuni sindaci si incatenano fisicamente alla sede della Protezione Civile regionale, qui a Palermo, per, in qualche modo, denunciare la gestione idrica.

                                E in tutto ciò, noi ci chiediamo, come sempre: 'E ma dove sono i dati che stanno dietro a queste scelte? Dove sono i dati che stanno dietro alle decisioni della riduzione idrica?'. E vediamo, intanto, se questi dati ci sono e come vengono pubblicati.

                                Eh, i dati a cui faccio riferimento sono relativi agli invasi a uso potabile, quindi essenzialmente le dighe la cui acqua contenuta all'interno è utilizzata per scopi potabili. Faccio una cosa simile a quella che ha fatto Giorgia, ma Giorgia l'ha fatto meglio, perché lei è Giorgia.

                                Ehm, ho chiesto a ChatGPT, ma anche a Gemini di Google, insomma... visto che abbiamo capito che è importante comprendere qual è il livello d'acqua contenuto, ad esempio, nell'Ancipa, che rifornisce alcuni comuni, chiedo: 'Cerca il volume invasato ad oggi dalla diga Ancipa, quella in Sicilia. Se non conosci il volume di oggi, almeno dammi quello più recente'. Ehm, ho abilitato però il 'deep search', quindi gli ho chiesto: 'Per favore, cerca in lungo e in largo per il web. Per favore, prova a darmi una risposta'.

                                Allora, che risposte mi ha dato? Gliel'ho chiesto esattamente stanotte a mezzanotte, non mi ricordo, una cosa simile. Comunque, oggi è il 13 giugno. OpenAI, quindi ChatGPT, mi ha detto: 'Il dato più recente è il primo maggio 2024'. Non è sicuramente oggi, è più di un mese fa. Google mi ha detto che i dati più recenti sono di febbraio del 2024.

                                Eh, non ne ha azzeccata nessuna delle due, perché, così come diceva Giorgia, i dati in realtà vengono pubblicati da un dipartimento, da una struttura regionale che si chiama 'Autorità di bacino del distretto idrografico della Sicilia', che ha un 'Osservatorio distrettuale permanente sugli utilizzi idrici'. Insomma, c'è questa struttura che pubblica, addirittura giornalmente da poco tempo, i dati relativi agli invasi a uso potabile.

                                Ora, so che tra il pubblico c'è la Regione con me. Non me ne vogliate se sto per dire... scherzo! No, no, no! Io vi voglio bene, giuro. Ma dobbiamo vedere... scherzo! Quelli che pubblicano così... Però, poi, sicuramente... ma siamo qui per... non me ne vogliate, veramente.

                                E perché la Regione li pubblica? E li pubblica così, giustamente, no? E io vedo Giorgia Lodi - voi non la vedete - che sta facendo così. E perché? Ma che abbiamo, Giorgia Lodi che fa così? Perché non sarò sicuramente io a dirvi - che sono la persona meno adatta - che i dati aperti sono classificati secondo alcuni livelli, le 5 stelle, eccetera eccetera, perché in questa stanza ci sono tutte le persone che hanno insegnato a me queste cose, quindi non sono io la persona adatta a dirle. E però, chiaramente, il PDF si colloca nel gradino più basso della scala, purtroppo.

                                E voi mi dite: 'Ah, vabbè, ma se questa è la scala dei dati aperti, allora un PDF è comunque un dato aperto'. Sì. Purtroppo, abbiamo qui anche chi nelle slide scrive: 'Sì, però i dati devono essere disponibili in formato aperto almeno a un livello pari a tre stelle'. E il livello pari a tre stelle, chiaramente, è un CSV, almeno un CSV.

                                Che si fa, allora? Solitamente, la roba che funziona sempre, insomma, è chiamare Borruso. Segnatevi il numero: 333 491 7592. No, sul serio, giornalmente riceve delle chiamate di questo tipo: 'Ciao, sono un giornalista. Scrivo per Il Sole 24 Ore. Mi sono imbattuto in un PDF di 200 pagine... devo scrivere però un articolo, devo analizzare questi dati, come faccio?'. Borruso è pronto, insomma, a rispondere, perché è presidente dell'associazione onData. In associazione, una cosa che fanno molto spesso è liberare i dati, no? Che sono intrappolati in PDF governativi. Ad esempio, uno su tutti: i dati sugli sbarchi. Vengono pubblicati, forse ancora oggi, con frequenza giornaliera, e noi facciamo l'aggiornamento quindicinale - ok, ogni 15 giorni - loro in modo giornaliero, ma non in formato aperto.

                                Ma anche la Sicilia, voglio dire, è abituata. E qui c'è Gabriele Scalici, che durante il Covid, insomma, ha estratto tutti i dati relativi all'epidemia da documenti PDF della Regione Siciliana. All'epoca non c'erano ancora gli strumenti, in termini di ChatGPT, LLM, di cui stiamo parlando oggi. Tutto veniva fatto in modo tradizionale. Cos'era, forse un Tesseract che usavi? Eh, sì, sì. Estraeva dati tabulari... prima faceva, diciamo, una lettura dati OCR, poi li rendeva tabulari. Un metodo, se vogliamo, 'a manina', tradizionale. Eravamo nel 2021.

                                Eh, chiaramente oggi sono cambiate un po' di cose. Forse quello che facevamo in modo tradizionale, e che per carità, continua a funzionare benissimo... anche se a volte con alcuni intoppi. E vediamo se riusciamo a fare una cosa simile, però sfruttando un sistema di intelligenza artificiale, quale può essere questo LLM.

                                Tutti gli esempi che vi farò vedere a partire da ora sono fatti usando il modello Gemini 1.5 Flash. Attenzione, lo sappiamo, cioè, stiamo utilizzando uno strumento mega potentissimo per fare un task veramente molto banale e che può essere raggiunto anche col metodo tradizionale. Il nostro è un esperimento, per capire se questo strumento può essere adottato anche per estrarre dati a partire da documenti non strutturati, come può essere un documento PDF.

                                E allora, che dobbiamo fare? Ad esempio, per l'emergenza idrica, potremmo costruirci il nostro algoritmo che cerca i documenti rilevanti, ci estrae i dati e poi li aggiorna, insomma, in continuazione.

                                Partiamo a identificare i documenti. Questo qui lo facciamo in modo tradizionale, perché è quello più semplice: ci colleghiamo al sito della Regione, ci sono diverse pagine che raccolgono, insomma, questi report in PDF, divisi per anno e per mese, e quindi semplicemente giungiamo al file PDF.

                                Dopodiché interviene quello che, in qualche modo, forse è lo step più importante, che è quello dell'estrazione dei dati. E qui facciamo tre richieste, facciamo formalmente tre chiamate all'AI di Google. Lui mi dirà: 'Perché? Ce ne basta una'. Ebbene, in realtà ne basta una, però ne facciamo tre perché vogliamo scongiurare al minimo l'eventualità dell'allucinazione. Dobbiamo ridurre al minimo il rischio: facciamo due estrazioni e un sanity check.

                                Cominciamo con la prima estrazione, che in realtà è la più difficile. Forniamo a Google Gemini la tabella con i dati non strutturati contenente tutti i volumi per ogni diga, insieme a un'anagrafica in CSV che contiene il nome della diga, la localizzazione, le coordinate, la descrizione dell'uso, eccetera eccetera. E il prompt è: 'A partire dai dati presenti non strutturati, estrai i nomi delle dighe, i volumi; addirittura, arricchiscili con il codice identificativo che non è presente nel PDF ma è presente nell'anagrafica. Quindi, fai una sorta di fuzzy match tra i nomi delle dighe nel PDF e nell'anagrafica, che non sempre corrispondono esattamente. Ovviamente, poi fai un join, insomma, ed estrai questo dato così come lo voglio io'.

                                Abbiamo il primo output, siamo contenti. Lo facciamo una seconda volta, stavolta senza l'anagrafica in CSV. Chiediamo solo: 'Estrai tutti i dati che trovi da questo documento PDF'. E otteniamo il secondo output.

                                A questo punto, facciamo il sanity check: confrontiamo i due output. Non facciamo il confronto sui nomi delle dighe, perché sappiamo che sicuramente i nomi differiscono, ma facciamo un confronto sui volumi. E l'output che troviamo sulla destra, che ci restituisce l'LLM, non è più un CSV come prima, ma è un report di valutazione in JSON. Ehm, questo JSON contiene solamente tre chiavi: ci dice se la validazione è stata superata, il motivo per cui è stata superata (o non è stata superata) e la data della validazione. E quindi questo qui è il check.

                                [Voce dal pubblico] Ho una curiosità: perché gli fai fare un controllo tra l'output della prima e l'output della seconda?

                                [Relatore] Perché la prima è 'guidata'. Cioè, perché nella prima c'è anche l'anagrafica e a volte si confonde tra i dati dei volumi nella diga e i dati che sono presenti nell'anagrafica. Nella seconda chiamata do solamente il PDF, senza anagrafica. Non t'ho convinto?

                                [Voce dal pubblico] No, no, io quando faccio così, gli faccio fare tre estrazioni.

                                [Relatore] Ah, invece di farne controllare due. Però, come si diceva prima, non c'è certezza, no?, su come usarli ancora. Semplicemente, faccio controllare le due estrazioni e poi faccio il report di validazione. Sì, sì, solo questo.

                                E questo processo si ripete per N volte, finché il report non è valido. N volte può essere 3, 4... È capitato che durante la prima iterazione uno dei due output era sbagliato, il report di validazione infatti lo segnalava, eh, e ripartiva da capo. Ed è capitato che dopo un paio di iterazioni - solitamente due, eh, ma ne basta in realtà molto spesso una - riusciamo ad estrarre i dati in modo corretto.

                                E dopo li aggiorniamo. Li aggiorniamo ogni giorno, perché se questi dati sono pubblicati ogni giorno, li aggiorniamo ogni giorno sul nostro repository di GitHub. E abbiamo una GitHub Action che esegue esattamente tutto quello che vi ho raccontato oggi.

                                Se non ricordo male... Andrea, ti posso chiedere di fare...? No, no, no, faccio io, perché pensavo fosse collegato. Vediamo se i dati di oggi sono stati estratti. Ehm, vediamo se... questa qui l'ha fatta durante il coffee break... eh... No, non so usare un Mac, scusate! Comunque, si sono estratti i dati, vediamo.

                                I dati che estraiamo li abbiamo anche documentati. Questo è 'come piace a Borruso'. Ma c'abbiamo anche i metadati in Frictionless, con lo standard del Data Package. Ed essenzialmente è tutto quello che facciamo. Poi abbiamo un bot che notifica sul canale di Open Data Sicilia... anche se poi ci siamo stancati delle notifiche giornaliere e le abbiamo staccate. Però funziona.

                                E possiamo ottenere delle cose di questo tipo: trasformiamo, così come aveva fatto Gabriele con i dati del Covid, un PDF, se vogliamo, in CSV, in API... E ci chiediamo: questi dati, qualcuno li sta riutilizzando?

                                Per fortuna, sì. Ad esempio, il Centro Meteorologico Siciliano usa come sorgente 'Dati Sicilia' per le sue valutazioni, i suoi monitoraggi sugli invasi idrici in Sicilia. E anche grazie a un altro membro di Open Data Sicilia che si chiama Giulio Di Chiara, che non so se in realtà oggi c'è. 

                                Consentitemi questa..

Viva il riuso, ma soprattutto, viva il Borruso!">Leggi la trascrizione</button>
                            </div>
                        </div>
                    </div>

                    <!-- Card Intervento 5: Borruso -->
                    <div class="bg-slate-800/50 backdrop-blur-sm border border-blue-800/50 rounded-xl p-6 flex flex-col justify-between shadow-lg hover:shadow-sky-500/20 transition-all duration-300">
                        <div class="flex-grow">
                            <h3 class="text-xl font-semibold text-white mb-2">Tutto è testo: perché la shell è il posto ideale per usare gli LLM</h3>
                            <p class="text-slate-400 mb-2">Andrea Borruso (onData e Open Data Sicilia)</p>
                            <p class="text-slate-300 text-sm mb-6">Una riflessione pratica su come lavorare con i Large Language Models direttamente dalla linea di comando, l'ambiente ideale per manipolare il testo.</p>
                        </div>
                        <div class="mt-auto pt-4 border-t border-slate-700/60">
                           <div class="flex flex-wrap items-center justify-between gap-3">
                                <div class="flex items-center gap-3">
                                    <a href="https://www.youtube.com/live/I8p-bEY_ago?si=7JR1k5yyuRDsK6-f&t=13388" class="text-center bg-sky-600 hover:bg-sky-700 text-white font-semibold py-2 px-3 rounded-lg transition-colors flex items-center justify-center gap-2">
                                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor"><path d="M2 6a2 2 0 012-2h6a2 2 0 012 2v8a2 2 0 01-2 2H4a2 2 0 01-2-2V6zM14.553 7.106A1 1 0 0014 8v4a1 1 0 00.553.894l2 1A1 1 0 0018 13V7a1 1 0 00-1.447-.894l-2 1z" /></svg>
                                        <span>Video</span>
                                    </a>
                                     <a href="https://aborruso.quarto.pub/tutto-testo/" class="text-center bg-slate-700 hover:bg-slate-600 text-white font-semibold py-2 px-3 rounded-lg transition-colors flex items-center justify-center gap-2">
                                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M3 17a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zm3.293-7.707a1 1 0 011.414 0L9 10.586V3a1 1 0 112 0v7.586l1.293-1.293a1 1 0 111.414 1.414l-3 3a1 1 0 01-1.414 0l-3-3a1 1 0 010-1.414z" clip-rule="evenodd" /></svg>
                                        <span>Slide</span>
                                    </a>
                                </div>
                                <button class="expand-trigger flex items-center justify-center gap-1.5 text-sm text-sky-400 hover:text-sky-300 font-medium transition-colors py-2 flex-shrink-0">
                                    <span>Approfondisci</span>
                                    <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 transition-transform chevron" viewBox="0 0 20 20" fill="currentColor">
                                        <path fill-rule="evenodd" d="M5.293 7.293a1 1 0 011.414 0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414 0l-4-4a1 1 0 010-1.414z" clip-rule="evenodd" />
                                    </svg>
                                </button>
                            </div>
                            <div class="expandable-content mt-4 space-y-2">
                                <button class="modal-trigger w-full text-left px-3 py-2 text-sm bg-slate-700/50 hover:bg-slate-700 rounded-md text-slate-300 hover:text-white transition-colors" data-title="Riepilogo: LLM e Shell" data-content="Andrea Borruso ha condiviso una riflessione pratica sull'uso dei Large Language Models direttamente dalla riga di comando, presentandola come l'ambiente ideale per la manipolazione del testo. Ha introdotto l'utility `LLM` di Simon Willison, uno strumento Python che facilita l'interazione con diversi modelli di LLM (come Antropic, Gemini, OpenAI, Ollama) direttamente dalla shell. Questo permette agli utenti, anche quelli non sviluppatori, di sfruttare la potenza dell'IA per compiti di elaborazione testuale e dati, dimostrando come gli LLM possano semplificare operazioni complesse, ad esempio, capire il funzionamento di un comando Unix come `tar` tramite un prompt in linguaggio naturale.

                                Borruso ha illustrato la versatilità di `LLM` per misurare i token (unità di costo negli LLM), gestire diversi modelli e impostarne i parametri come la 'temperatura' (che influenza la creatività delle risposte). Ha mostrato come, essendo la shell un ambiente orientato al testo, sia possibile passare input testuali (come l'output di comandi di sistema o pagine web intere) direttamente all'LLM e ricevere risposte strutturate. Ha fornito esempi pratici di scraping web basato sul linguaggio naturale (es. estrazione di notizie da un sito), generazione di dati fittizi in formato JSON e applicazione di schemi predefiniti per estrarre e normalizzare informazioni da pagine web complesse, come i dati dei consiglieri comunali.

                                Un aspetto innovativo dimostrato è stata la capacità di 'reverse engineering' di schemi dati: Borruso ha mostrato come l'LLM possa descrivere un file CSV secondo lo standard Frictionless Data, fornendo un file JSON che ne dettaglia la struttura e i tipi di campo. Ha anche presentato l'uso di template per normalizzare indirizzi o chattare con contenuti video estraendo sottotitoli e sintetizzandone i punti chiave. L'intervento ha sottolineato come questi strumenti rendano l'IA accessibile e potente per chi lavora con i dati, facilitando l'esplorazione, la strutturazione e l'analisi anche per chi non ha competenze di programmazione avanzate, promuovendo al contempo la standardizzazione e la qualità dei dati.">Leggi il riepilogo</button>
                                <button class="modal-trigger w-full text-left px-3 py-2 text-sm bg-slate-700/50 hover:bg-slate-700 rounded-md text-slate-300 hover:text-white transition-colors" data-title="Trascrizione: LLM e Shell" data-content="Allora, quanti di voi hanno più di cinque amici diventati esperti di intelligenza artificiale? 10? Io penso pure io. Io non lo sono. Oggi vi racconto un'esperienza da utente e perché, dal punto di vista di chi utilizza la riga di comando per stare su operazioni di lettura e trasformazione dati, da utente, ti cambia le carte in tavola.

                                Io mostrerò uno strumento in particolare perché, secondo me, è lo strumento che più sembra uno strumento di base, come nella shell sono `awk`, `less`. È uno strumentino molto orizzontale con cui ci si interfaccia a qualsiasi, eh, modello: modello locale, modello via API. È molto facile da utilizzare, chiaramente, se si sta dentro questi contesti. E inizio, sennò bla bla bla.

                                Allora, iniziamo da una cosa buffa ma molto vera. Eh, non è mia, è una bellissima vignetta di xkcd, eh, in cui due amici si parlano. C'hanno la bomba atomica che sta per scoppiare (in questi giochi tra gli argomenti fa un po' schifo, ma comunque...). Solo che, per bloccare il conto alla rovescia, devi saper utilizzare il mitico comando `tar`, e ogni volta devi andare a leggere il manuale, ricordarti, cercare nella history... e difatti la vignetta finisce con la roba che, insomma, è stata scoppiata. Perché, comunque, ricordarsi come funziona `tar`, obiettivamente è impossibile.

                                C'è poco da ridere, nel senso che, eh, facciamo un salto indietro di qualche anno, quando non c'era l'intelligenza artificiale. Non lo so dire, non so perché. Cercavamo su Stack Exchange, cercavamo su Google, su Reddit, chiedevamo all'amico &quot;psicotico&quot;, quello bravo che sapeva fare. &quot;No, io a questo non ci arrivo&quot;. &quot;Non è cosa, psicotico!&quot;. &quot;Sì, ma non sapevo questa cosa&quot;. &quot;Ci arrivi, dai!&quot;. Esatto. Ragazzi, quella è un'altra vignetta meravigliosa. Oppure, utilizzare `man`. Apriamo la riga di comando. Se io faccio `man tar`, quindi il manuale di `tar`, eh, mi leggo pagine e pagine, devo andare a capire il mio caso specifico. Vabbè, diciamo, se divento un utente esperto, ci mancherebbe, è un comando molto facile. Per un utente una tantum, la vignetta funziona molto bene.

                                Ehm, ma ora c'è l'intelligenza artificiale e non scoppiano più le bombe. Vabbè, diciamo, in realtà si pigliano sempre cantonate se si usa male, soprattutto. Eh, ma a quanti token corrisponde questo manuale? Il token è importante conoscerlo, no? È l'unità di misura dello scambio dati tra noi e i motori di intelligenza artificiale. Il token lo paghiamo. Ci sono modelli che hanno un *free tier* e, quindi, un certo numero di token gratuiti in ingresso e in uscita, e iniziare a prendere le misure di queste cose diventa essenziale quando poi si vuole andare sia a testare, sia ad andare in produzione. Diciamo che i token, più o meno, sono tre-quattro lettere, una parola... dipende un po' dal modello di tokenizzazione. Ma, per esempio, con questo strumentino a riga di comando che si chiama `tiktoken`, è molto comodo perché passi direttamente in *pipe* qualsiasi contenuto testuale: un XML, un Markdown, un CSV, un JSON. `pipe tiktoken` e hai il numero di token.

                                L'altra cosa carina che puoi fare, eh, gli puoi dire: &quot;Dammi i primi 1000 token&quot;. Questo ti può servire perché, se sai che stai usando un modellino che ha dei limiti in ingresso, devi capire se il tuo ragionamento è compatibile con quel numero di token. Simon Willison è, per me, la persona di riferimento più importante nel mondo dell'intelligenza artificiale. È l'autore di `tiktoken`, ma è l'autore dell'utility che vedremo tra poco, che si chiama LLM. Sembra un nome di fantasia ma, diciamo, è un nome molto, molto appropriato. E se non lo conoscete, iscrivetevi alla sua newsletter e al suo blog: è straordinario.

                                Lavorando alla vecchia maniera... scusate, è il fatto di essere l'ultimo e che è tardi. Allora, chiaramente, alla vecchia maniera, se io mi misuro con il buon `wc` del manuale, quindi &quot;contami le linee&quot;, sono 956 linee di manuale. E — vado alle slide, non faccio tutti i comandi — sono 5.190 parole e 9.008 token. Insomma, poca roba, nel senso che la gran parte dei modelli e delle API che utilizziamo di più stanno di base su 128.000 token in ingresso. Ci sono adesso *free tier* con 1 milione di token, con una chiamata gratis ogni, credo, 4-5 secondi, e poi per utilizzi intensi diventano pochi anche quelli.

                                E a quel punto utilizzo l'utility che si chiama LLM. L'utility di Python si installa con `pip` o con `uv`, e gli passo i dati tramite *standard input*/*standard output* — una cosa vecchia quasi 50 anni, insomma, parliamo dei primi computer Unix, Linux. Vi faccio la domanda, il classico *prompting*, come si usa. Prendo come input tutto il manuale di `tar`, perché io non so niente, lo passo a LLM e gli chiedo in linguaggio naturale: &quot;Crea un archivio `tar` dal file `appunti.tar.gz`. Scrivi soltanto il comando e commenta le opzioni&quot;. Se non gli scrivo questo, mi vomita il papello della storia di `tar`. Quindi il *prompt engineering*, scrivere bene le domande, ovviamente è fondamentale, è molto più importante degli strumenti.

                                Eh, qual è la risposta? Mi dice: &quot;Guarda, devi utilizzare `tar -xzf`&quot;. Me lo dimentico sempre. È carino perché, poi, mi fa l'elenco puntato in Markdown, quindi può diventare una guida HTML in cui mi dice che il parametro `-x` fa questo, `-z` fa quello... e quindi rimango nel mio contesto operativo, che è la shell, e rilancio il comando subito dopo. E, appunto, sempre *standard output* e *standard in*, quindi tutto chiavi in mano, funzionale.

                                L'utility si chiama LLM, non come il concetto di *large language model* (ovviamente cita quello), e l'autore è Simon Willison. La cosa bella è che lo potete utilizzare come libreria Python, quindi se vi piace come funziona, se vi sembra veramente *easy*, lo potete mettere in *pipeline* di codice Python più robuste e complesse, con più verifiche di errore rispetto a un classico script Bash. È anche il creatore di Datasette, che per noi che ci occupiamo di dati è un'applicazione straordinaria per pubblicare dati ed API su interfacce web.

                                Esistono decine di plugin per LLM. Tendenzialmente, la gran parte serve ad aggiungere modelli o interazioni con API. Da pochissimo hanno rilasciato le API per parlare con i modelli disponibili in GitHub Copilot, che sono decine. Anche qui avete il *free tier* o, se siete abbonati, avete i token del vostro abbonamento. I modelli si elencano col comando `models`, e questi che vedete sono quelli installati sulla mia macchina: Anthropic, Gemini, OpenAI, Ollama. Potete anche lavorare con modelli in locale se avete una macchina che ve lo consente.

                                Il modello di default è GPT-4o, ma basta che vi ricavate delle chiavi API e potete usare quello che volete. La cosa comoda è che, potendo usare 50-60 modelli, posso fare dei test per capire qual è il migliore per certe cose. Chiaramente, è inutile fare a tentativi: bisogna studiare cosa fanno i modelli rispetto al nostro obiettivo. Questo è un esempio didattico: faccio un semplice loop su tre importanti LLM generalisti (Claude Sonnet, Gemini 1.5 Flash e ChatGPT-4o) e gli dico: &quot;Dammi un nome per un convegno su...&quot;. Lo mando in loop e ottengo tre risultati. Niente di emozionante, ma pensate ai casi operativi veri.

                                La cosa interessante è che potete impostare i parametri, come la temperatura. Su ogni modello, potete guardarvi le opzioni. Qui stiamo guardando Gemini 1.5 Flash: espone la temperatura (quanto è fantasioso), il numero massimo di token in output, se abilitare Google Search, se usare file allegati (GIF, PDF, MP3). Ogni modello ha le sue specifiche. Per curiosità, vi faccio vedere un altro modello, quello di OpenAI: anche qui abbiamo la temperatura, che ti dice può variare tra 0 e 2. Nelle opzioni di Gemini non c'è scritto, bisogna guardare la documentazione. Grande facilità: ho fatto un loop in 2 secondi, ho parlato con tre modelli e posso usare parametri seri per bilanciare le mie richieste.

                                Ma torniamo alla shell. Il comando `lsb_release -a` ti stampa il sistema operativo. Io ho installato Debian dentro WSL 2. Lui mi dice: `Description: Penguin`, `Release: 12`. Chi fa questo mestiere non ha bisogno di spiegazioni, ma è interessante che, essendo nella shell, è tutto testo e posso usare comandi da *subshell*. Quindi, lancio un *prompt*: &quot;Quali sono i punti chiave del mio OS?&quot;. Il comando viene eseguito, l'output passato a LLM che mi risponde: &quot;I punti chiave sono: è una distribuzione Debian, versione 12, nome in codice Bookworm...&quot;. Di default, l'output è in Markdown, quindi facilmente trasformabile in HTML.

                                Il web è testo, una delle principali fonti per fare cose. Vi faccio vedere questo esempio. &quot;Sì, eseguila! Faccela vedere!&quot;. Ve la faccio vedere. Allora, sto chiedendo sull'homepage de *Il Post* quali sono le categorie di notizie più presenti: Israele ed Iran, Elly Schlein, cronaca, ecc. Un comandino `curl` manda tutto il contenuto, anche CSS e JavaScript, sprecando un po' di token. Lui è un LLM e mi risponde in linguaggio naturale.

                                Potrei fare una specie di *scraping* ma parlandogli. Voglio analizzare solo la sezione &quot;Storie e Idee&quot;. E allora, dialogo. Gli dico: &quot;Estrai soltanto i titoli della sezione Storie e Idee&quot;. E lui li estrae: &quot;I nostri insegnanti si ricordano di noi&quot;, &quot;Dieci figli all'estero&quot;, ecc. Non sto facendo XPath o selettori CSS. Potrei anche fare una ricerca semantica, per esempio sulla parola chiave &quot;Palestina&quot;, e lui troverebbe titoli correlati, mappando anche &quot;Israele&quot;.

                                Io però voglio testo strutturato, e allora aggiungo l'opzione `--schema-multi` e chiedo &quot;titolo&quot; e &quot;URL&quot;. Devo fare un bot che ogni giorno estrae questi dati per un'analisi. Ed è eccezionale... nel senso che... zero. Questo è il bello della diretta. Se avesse funzionato, avreste visto titolo e URL. Fidatevi, vi faccio altri esempi.

                                Il testo strutturato lo posso generare anche per creare dati *fake*. (&quot;Ci segnalano che passi davanti alla webcam&quot;. &quot;Ma digli che è gratis!&quot;. Vabbè, sto fermo). Adesso genero dati *fake*. Con `--schema-multi` do io uno schema fittizio: nome, città, indirizzo, posti a sedere (come intero, `int`). Sto creando cinque ristoranti italiani fittizi e ottengo il JSON. Nel JSON, `posti_a_sedere` è un numero intero, senza virgolette. È comodissimo poter generare dati *fake*.

                                Guardiamo quest'altro, ancora più carino. Voglio ricavare i nomi dei consiglieri comunali d'Italia. C'è da spararsi, perché i siti sono tutti diversi. Siamo a Palermo. Obiettivo: estrarre nome, cognome, URL della pagina del consigliere e il suo ruolo. Ho scritto un comando in cui passo con `curl` l'URL del sito e dettaglio lo schema: `nome: il nome del consigliere`, `ruolo: il ruolo`, `url: l'URL della pagina`. E poi un dato calcolato: il sesso. In pagina non c'è, ma lui, leggendo &quot;Mario&quot;, metterà &quot;m&quot;. Ho previsto &quot;m&quot;, &quot;f&quot; e &quot;na&quot; (per dubbio/inapplicabile). Il *prompt* è: &quot;Estrai i dati dei consiglieri comunali presenti nella pagina web&quot;. Sta funzionando. Molto interessante: ha fatto *scraping* e ha compreso da solo nomi e ruoli, trovando persino una &quot;consigliere comunale dimissionaria&quot;.

                                (&quot;Domanda da casa: visto che c'è la foto, potrebbe stabilire il genere analizzandola?&quot;). Ottima domanda. Sì, si può fare e funziona. Possiamo passargli la foto come allegato o lo *screenshot* della pagina. (&quot;L'output in JSON non glielo chiedi tu?&quot;). No, è nativo, è il formato più comodo e quello che le API degli LLM usano. Trasformarlo in CSV è facile. I dati sono lì. Questo è un JSON piatto che posso esplorare con strumenti come `jq`. Sono a riga di comando, posso trasformare e leggere subito.

                                (&quot;Per fare questa estrazione devo usare per forza un token a pagamento?&quot;). No, il *free tier* di Gemini va benissimo. La cosa carina è che quando scrivo `nome`, `ruolo`, `url`, sto definendo uno schema. Ogni schema ha un ID e una mappatura. La mappatura *full* è tipo JSON API, parlante. Questo schema lo posso usare per controlli e rettifiche. È *human-readable* e *machine-readable*.

                                La cosa bella? Ho creato questo schema, e ora lo uso per un altro comune. Andiamo a Enna. Il sito è simile, ma non uguale. Utilizzo lo stesso schema, passandogli l'ID. Fantastico! Chiaramente, funzionerà su 10 siti su 8.000, ma con una buona progettazione su *task* verticali, potete riutilizzare gli schemi.

                                Esempio dell'immagine, per Totò. Passo il *template* &quot;consiglio comunale&quot; e con `--attachment` gli do una PNG. L'input è un'immagine con sei consiglieri. Lo schema è lo stesso, ma il lavoro è diverso: è come se facesse OCR. Otteniamo lo stesso schema, ma senza l'URL, che non si può recuperare da un'immagine.

                                Ora descriviamo una tabella. Creiamo una tabellina *fake* di cinque persone, la trasformiamo in CSV. Anno, mese, giorno, età come numero intero. Mi sono generato il dato fittizio. L'obiettivo è descriverlo. Gli chiedo: &quot;Descrivimi in modalità Frictionless il file `persone.csv`, usando queste specifiche&quot;. Le specifiche gliele passo io. `cat frictionless.json`. Questo è il JSON con le specifiche Frictionless, uno standard internazionale per descrivere dati, citato anche nelle linee guida *open data*. Lui fa *reverse engineering*: guarda il CSV e lo descrive secondo lo standard. Il comando ha di speciale `-f frictionless.json`. È come fare un RAG: &quot;Per rispondere, tieni conto di questo testo&quot;.

                                (&quot;Scusa, e se due persone hanno lo stesso cognome? Ho visto che ha messo `primaryKey: cognome`&quot;). È un'allucinazione. Lui ha pensato: &quot;Sarebbe carino avere un ID&quot;. Ha pensato che il cognome fosse univoco, ma non lo è. Da operatore, devi correggere lo schema. È uno dei problemi di questi strumenti.

                                Lui dà un titolo, &quot;People data&quot;, descrive il dataset, capisce che `age` è un `integer`, che `birthDate` è `type: date` e ne indovina il formato. Ora ritorno in chat. Posso andare in continuità con `llm -c`. Lui si ricorda il contesto e gli dico: &quot;Ora descrivilo per una tabellina su un sito web&quot;. E mi estrae il Markdown da mettere nel mio portale *open data*.

                                È possibile salvare *template* e *prompt* per operazioni ricorrenti. Esempio stupido: normalizzare indirizzi stradali scritti in modi diversi. Mi sono costruito un *prompt* verboso in cui gli dico: &quot;Cerca il pattern degli indirizzi, estrai il tipo di strada, l'odonimo, il civico&quot;. Se lo istruite bene, funziona meglio. Queste sono le mie strade, scritte a casaccio. Usando il *template* e passandogli il file `.txt` in *pipe*, mi estrae le informazioni in modo strutturato. Attenzione, è didattica. Con milioni di *record* non si può fare così, bisogna progettare meglio.

                                Ho quasi finito. Applicazioni: chattare con un video. C'è NotebookLM, ma mi sono fatto una piccola applicazione. Gli do l'URL del video e una domanda. Uso un *template* &quot;handy&quot;, che vuole una risposta sintetica. Posso chiedergli di rispondere in italiano, anche se il video è in un'altra lingua, una magia degli LLM. Vi faccio un esempio: Tim Berners-Lee che parla di Linked Data. Lui dialoga con YouTube, scarica i sottotitoli e mi risponde. I punti chiave sono: &quot;frustrazione con i sistemi attuali&quot;, &quot;i principi&quot;, &quot;l'importanza della connessione&quot;. Tim è sempre Tim.

                                Perché ve ne parlo? Perché, non essendo uno sviluppatore, ho messo in piedi un po' di codice con un *system prompt*: &quot;Sei un sistema che mi assiste nel rispondere a domande su video YouTube&quot;. Nei *template* posso usare variabili, come `language`. Infatti, posso chiedere la stessa risposta in francese. È un po' più breve, non so perché, ma è in francese.

                                E fine. Passo la palla ad Andrea, Nelson, Mauro.">Leggi la trascrizione</button>
                            </div>
                        </div>
                    </div>

                    <!-- Card Intervento 6: Mauro -->
                    <div class="bg-slate-800/50 backdrop-blur-sm border border-blue-800/50 rounded-xl p-6 flex flex-col justify-between shadow-lg hover:shadow-sky-500/20 transition-all duration-300">
                        <div class="flex-grow">
                            <h3 class="text-xl font-semibold text-white mb-2">Creare scraper in python senza saper usare python</h3>
                            <p class="text-slate-400 mb-2">Andrea Nelson Mauro (onData e Open Data Sicilia)</p>
                            <p class="text-slate-300 text-sm mb-6">L'intervento illustra come sfruttare i Large Language Models (LLM) per creare scraper in Python e automatizzare l'estrazione dati, anche per chi non sa programmare.</p>
                        </div>
                        <div class="mt-auto pt-4 border-t border-slate-700/60">
                           <div class="flex flex-wrap items-center justify-between gap-3">
                                <div class="flex items-center gap-3">
                                    <a href="https://www.youtube.com/live/I8p-bEY_ago?si=M14tfvPG83LGY_Rl&t=15713" class="text-center bg-sky-600 hover:bg-sky-700 text-white font-semibold py-2 px-3 rounded-lg transition-colors flex items-center justify-center gap-2">
                                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor"><path d="M2 6a2 2 0 012-2h6a2 2 0 012 2v8a2 2 0 01-2 2H4a2 2 0 01-2-2V6zM14.553 7.106A1 1 0 0014 8v4a1 1 0 00.553.894l2 1A1 1 0 0018 13V7a1 1 0 00-1.447-.894l-2 1z" /></svg>
                                        <span>Video</span>
                                    </a>
                                     <a href="https://github.com/datapitch-it/tables" class="text-center bg-slate-700 hover:bg-slate-600 text-white font-semibold py-2 px-3 rounded-lg transition-colors flex items-center justify-center gap-2">
                                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M3 17a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zm3.293-7.707a1 1 0 011.414 0L9 10.586V3a1 1 0 112 0v7.586l1.293-1.293a1 1 0 111.414 1.414l-3 3a1 1 0 01-1.414 0l-3-3a1 1 0 010-1.414z" clip-rule="evenodd" /></svg>
                                        <span>Repository</span>
                                    </a>
                                </div>
                                <button class="expand-trigger flex items-center justify-center gap-1.5 text-sm text-sky-400 hover:text-sky-300 font-medium transition-colors py-2 flex-shrink-0">
                                    <span>Approfondisci</span>
                                    <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 transition-transform chevron" viewBox="0 0 20 20" fill="currentColor">
                                        <path fill-rule="evenodd" d="M5.293 7.293a1 1 0 011.414 0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414 0l-4-4a1 1 0 010-1.414z" clip-rule="evenodd" />
                                    </svg>
                                </button>
                            </div>
                            <div class="expandable-content mt-4 space-y-2">
                                <button class="modal-trigger w-full text-left px-3 py-2 text-sm bg-slate-700/50 hover:bg-slate-700 rounded-md text-slate-300 hover:text-white transition-colors" data-title="Riepilogo: LLM e Shell" data-content="Andrea Nelson Mauro ha proseguito la discussione sull'accessibilità dell'IA, dimostrando come sia possibile creare strumenti complessi, come scraper in Python, senza possedere una conoscenza approfondita del linguaggio, grazie all'assistenza dei Large Language Models. Ha raccontato la sua esperienza personale, dove, partendo da zero in Python, è riuscito a generare una decina di scraper per monitorare bandi e progetti europei, semplicemente dialogando con un LLM. Il modello non solo ha scritto il codice, ma lo ha anche commentato e spiegato, accelerando notevolmente la curva di apprendimento e i tempi di sviluppo. Questo approccio permette di imporre standard di output (es. formattazione delle date) per garantire l'uniformità dei dati estratti da fonti diverse.

                                Un altro caso d'uso significativo presentato da Nelson Mauro riguarda l'estrazione di movimenti bancari da PDF non strutturati, un compito notoriamente difficile per gli strumenti tradizionali. L'LLM è riuscito a estrarre centinaia di pagine di dati in pochi minuti, dimostrando l'enorme potenziale di questi strumenti nella strutturazione di informazioni complesse e disordinate. Questa capacità di trasformare qualsiasi input in un formato strutturato, guidato da istruzioni specifiche, è stata sottolineata come uno dei punti di forza dell'IA generativa nel campo dei dati.

                                La presentazione si è conclusa con una riflessione sulle implicazioni di queste tecnologie per la pubblica amministrazione e la democratizzazione delle competenze data-related. Sebbene gli LLM non siano una soluzione universale e richiedano un certo livello di formazione, possono abbassare la barriera d'ingresso per molte attività di analisi e manipolazione dati, aprendo possibilità a un pubblico più ampio di persone. Tuttavia, è emersa la necessità di un forte 'volere' da parte della PA e di investimenti in budget e formazione, poiché la qualità dei dati di input e la capacità di valutare criticamente l'output generato dall'IA rimangono cruciali per evitare problemi di affidabilità e bias, soprattutto quando le decisioni hanno un impatto sulla vita dei cittadini.">Leggi il riepilogo</button>
                                <button class="modal-trigger w-full text-left px-3 py-2 text-sm bg-slate-700/50 hover:bg-slate-700 rounded-md text-slate-300 hover:text-white transition-colors" data-title="Trascrizione: LLM e Shell" data-content="Io, in realtà, faccio soltanto una cosa molto rapida. Borruso lo conosciamo, è uno bravo, no? Ma quelli scarsi, che usano, cosa possono fare? Faccio vedere... i finti modesti sono terribili. No, io... io non sono... se lui, che non è uno sviluppatore... figuratevi io. Allora, vediamo se... ecco.

                                Allora, io per lavoro, tra le varie cose, mi occupo di progettazione: progettazione europea, bandi, bla bla bla e tutto questo mondo qui. Questo qui è un visualizzatore di tabelle, banalissimo HTML, in cui ci sono dati su progetti europei o bandi, cose, no? Io li devo monitorare, ce ne sono mille di fonti e quindi mi sono detto: «Ah, vediamo se è vero quello che dice Borruso. Provo a farne uno».

                                Sono andato su un portale, ho provato a usare l'LLM, mi sono fatto il mio scraper — adesso ve li faccio vedere. Alla fine ne ho fatti, cosa sono? Dieci. C'è un sito della Regione Emilia-Romagna, alcune fonti dati europee, in Italia One-Pass, INPAT... ci sono dei bandi, un sito che pubblica bandi sul mondo del giornalismo, Regione Lombardia e New Items, invece, è un filtro dei nuovi.

                                Faccio vedere che cosa ha fatto lui o lei, non so come chiamarlo. Questo era il sito dove ho messo tutto. Praticamente, facendo il gioco che ha fatto Andrea, cioè: lui è bravo, lo fa per strutturare i dati; io, invece, i dati non ce li ho, lo faccio per fare scraping. Quindi, banalmente, entri sulla pagina, copi il DOM (quindi l'HTML), lo prendi e lo dai all'LLM: «Voglio fare lo scraping di questi dati qui, di bandi che ci sono, che hanno questa caratteristica, no? Che hanno un titolo, eccetera». E lui mi ha creato — ve ne faccio vedere uno, ma, in realtà, poi mi ha guidato — mi ha creato lui da solo, personalizzato. Ovviamente c'erano dei bug che gli dovevo dire: «Questo non va bene, fai di là, eccetera». Mi ha creato 10 scraper in Python. Io non avevo mai scritto una riga di Python prima, tranne quando ho comprato il mini pocket, che ho fatto i primi giochetti, poi c'era da scaricare... Noi da Milano siamo veloci, solo fatturati, quindi 'sta cosa era da troppo lontano, poi dovevo studiare. Invece l'LLM me l'ha creato.

                                Quindi mi importa le librerie, mi definisce tutto, scritto soprattutto in modo che me lo commenta. Quindi quel poco che sapevo prima di Python l'ho anche un pochettino aumentato, perché oltre ad avere la soluzione, me l'ha anche spiegato. Quando dicono, no?, si leggono, si vedono 'ste interviste di gente che dice: «Eh, l'intelligenza artificiale può sviluppare software». Mistica. Nel mio caso, io ero a zero, cioè, facevo HTML eccetera, Python non l'avevo mai usato. E non solo. Poi io gli do insieme tutti gli script che ha creato e gli dico: «Voglio che lo standard... perché comunque leggo le cose che scrive Andrea... voglio che i dati in output abbiano lo stesso standard, no? Quindi che tutte le date siano formattate allo stesso modo», perché io poi li metterò in un sistema. E questo, quindi, è l'altro lato.

                                Poi il repo, se vi interessa, ve lo condivido. Comunque, in sostanza, partendo da zero, senza mai aver usato questo LLM, quindi semplicemente rompendo un po' le scatole come Dennis, ma niente di particolare, mi sono messo a studiare. Ho trovato un po' di tempo, ho dedicato un po' di tempo e mi sono creato tutto il mio sistema. Quindi, vabbè, con una paginetta HTML, uno script-ino che mi gestisce questi file, gli script Python che mi scaricano i file, faccio una &quot;run&quot; e mi scaricano tutti i dati quando voglio io. Quindi ogni mattina gli dico: «Fai, scarica tutto». Preferisco controllarla a mano, però, diciamo, ogni mattina premo un pulsante, fa tutto, mi aggiorna la pagina, aggiorno online la versione dei bandi e non ho bisogno di andare su tutti 'sti siti a cercarmi quali sono gli ultimi.

                                Quindi questa non è una versione, diciamo, narrativa, non dimostrativa, ma quello che ha fatto vedere lui prima non è che è la punta dell'iceberg. Molto banale.

                                Una domanda: quanto ci hai messo a costruire tutto questo?

                                All'inizio smadonnavo un po', perché, per quanto tu possa fare l'esperto e sentirti consapevole, in realtà entrare nelle dinamiche... ma banalmente, capire anche, che ne so, i path dell'XPath, del CSS, no? Adesso ci metto un'ora. Cioè, una cosa che prima non sapevo fare, per cui ci avrei messo un mese per imparare a fare uno script Python, adesso, dopo un po' di esercitazione, ci metto un'ora. Quindi io non so dove ci porta 'sta roba, però sicuramente ci sono tutta una serie di cose che prima era impensabile si potessero fare e oggi invece si possono fare. E io lo trovo allucinante.

                                Un altro caso d'uso banale: dovevo fare un'analisi di dati sul conto corrente. Incredibile, l'estratto conto di un anno. Ne parlo, ovviamente, come tutti noi italiani, scrivo ad Andrea. Lui comincia a dire, giustamente: «Guarda che ci sono delle librerie anche fighissime...». Poi le ho provate, e via dicendo. Modalità tradizionali, come si diceva prima. A un certo punto, però, lui giustamente dice: «Ma perché non lo facciamo fare all'AI?». C'è il tema dei dati personali, però con gli LLM puoi avere anche dei modelli in locale, più lenti, più leggeri. C'è tutto un dibattito tra locale e cloud, eccetera. Insomma, alla fine mi ha estratto, col formato che volevo io, 90 pagine di movimenti bancari di un anno, qualsiasi cosa, da questi PDF stranissimi, fatti in maniera schifosa, che anche i sistemi tradizionali non riuscivano a leggere perché c'erano i pattern sovrapposti. I PDF sono un po' complicati, diciamo. Me li ha estratti in, boh, 5 minuti. 5 minuti! Una roba che io... Quindi la potenzialità è effettivamente straordinaria, soprattutto, mi sembra, nella strutturazione del dato. Questa cosa qui mi sembra molto rilevante, cioè il fatto che tu gli butti dentro qualcosa, qualsiasi cosa, e guidato in un certo modo, con i &quot;guardrails&quot;, no?, come dicono quelli bravi, se tu gli dici &quot;se c'è questa cosa&quot; e gli dai una forma, un formato... non so come dirlo, però per me m'ha cambiato la vita.

                                Per esempio, io utilizzo, faccio interrogazioni alle API del portale nazionale e quando non lo butti giù... No, no, è una leggenda. [Musica] Ed è interessantissimo perché l'output JSON alle volte non mi serve, mi piace un output discorsivo perché voglio semplicemente ragionare su un'idea. Mi stoppo su 100 risultati per evitare problemi di token, ma è comodissimo. Diciamo, la prima analisi che noi facciamo, noi che lavoriamo con i dati, ti apre delle porte incredibili.

                                Altro intervento:

                                C'è tutta questa prima analisi di esplorazione con questi motori, che sono tanti, perché attenzione: OpenAI, Anthropic, Mistral, hanno tutti dei prodotti a riga di comando con cui fare cose, anche cose molto più avanzate. Però questo, secondo me, è uno strumento molto generalista, molto orizzontale, molto comodo, molto pronto all'uso. Io lo uso su Linux, ma è Python, quindi si installa anche... Sì, l'ho installato pure su Windows allo stesso modo. Non solo, puoi metterci dentro tutti i modelli che vuoi, è un wrapper di modelli. E io, il mio default, è Mistral, perché comunque io ancora ci vedo, nell'Europa, un pochettino... e per quanto siano francesi, il mio default anche sul telefono è Mistral. Quindi tutte le cose di base le faccio con Mistral, ma funziona anche perché l'approccio a schema è nativo su tutti gli LLM, su tutti i motori, e quindi lo puoi fare su Mistral, su Gemini o &quot;Gèmini&quot;, come lo chiamate voi.

                                Nuovo intervento:

                                Stef, vorrei chiedervi un'opinione, a parte il fatto che voi siete tutti scarsissimi, a partire da Borruso. Vabbè, ma che io sia più scarso di Borruso penso che sia ovvio, cioè, tautologico. A parte questo, volevo sapere, visto che comunque, dagli interventi che abbiamo visto precedentemente, no?, il problema è la produzione dei dati, il processo dei dati. Questo strumento, o comunque questi nuovi strumenti che avete illustrato, penso possa essere un acceleratore in questo senso. Poi, appunto, volevo sapere la vostra opinione se effettivamente possono essere un acceleratore nei confronti di una PA che, evidentemente, ha in quel punto lì il granello che fa bloccare tutto.

                                Risposta:

                                Dico la mia, poi secondo me Antonio e Giorgia sono meglio su questo. Secondo voi potrebbe essere questo, Andrea, un qualcosa che riesce a sbloccare? Ovviamente non sono strumenti per tutti, però io penso che con un'adeguata formazione e con adeguati fondi, magari, possano aprire possibilità anche a tutta una platea di persone che, con un'adeguata formazione, possono poi arrivare a fare cose che prima invece erano dedicate solo ai data scientist o comunque a persone che avessero un background tecnico-scientifico, che ovviamente non poteva essere appannaggio di chiunque nella Pubblica Amministrazione. Potrebbe essere questa una chiave di volta, secondo voi?

                                Altra risposta:

                                Secondo me, il punto prima è: &quot;lo vuoi fare?&quot;. E parlo di quella regione, nel senso, gli strumenti sono eccezionali. Se ti metti con obiettivo la qualità, l'automazione, i processi messi sotto controllo... per esempio, fare controlli automatici o scrivere codici per fare controlli automatici. L'oggettino che crea la descrizione &quot;frictionless&quot; poi puoi utilizzarlo per fare controllo, perché se il dato va fuori schema, quello schema te lo controlla. Quindi la risposta è sì, però, secondo me, il punto è se lo vuoi fare. E non vale solo per la PA, vale per tutti noi: aziende, cittadini. Quindi per me la risposta è questa, non so che ne pensano Giorgia e Antonio. Però, &quot;se lo vuoi fare&quot; e &quot;se c'è budget&quot; — l'altro punto che hai detto e non è da poco. Perché mi sembra che l'AgID, qui rappresentata... non so loro due, ma sembra di capire che lo staff sia poco rispetto agli obiettivi importantissimi che hanno.

                                Altro intervento:

                                Vorrei dare una risposta diversa. Diciamo, io sto facendo dei corsi sull'uso di questi strumenti di intelligenza artificiale da &quot;practitioner&quot;, diciamo così, tecnicamente, non certo dall'aspetto teorico, però studio l'argomento da un bel po', specificamente da oltre un anno e mezzo. Allora, dopo un corso, mando un messaggio alla classe, parliamo di LLM, e in Lombardia mi ha risposto un tizio. Mi ha detto — e guardate che quello che sta dicendo 'sto tizio è paro paro quello che diceva prima Andrea, no? — «Il manuale... mi si è rotto il forno». Mi ha trovato il manuale di fabbricazione, se l'è mangiato, gli ho detto cosa non funzionava, gli ho dato una foto del display, mi ha dato due possibili cause e tre soluzioni, dalla più soft alla più alta. Per ora funziona e non ho chiamato il tecnico. Cioè, io penso che questa cosa qua abiliti chiunque, potenzialmente. Dipende sempre da come usiamo gli strumenti.

                                Discussione finale:

                                * Voce 1: Però c'è un tema di budget. Nel senso che cinque anni fa fare queste cose significava formare del personale con determinate tecniche e con determinati strumenti che avevano una certa richiesta di conoscenza. Ora la richiesta di conoscenza c'è, ma con la voglia, la forbice si allarga un po' di meno.
                                * Voce 2: Penso che la formazione... la rampa è sicuramente... no, è come con la ricchezza: quando tutti diventano più poveri, i poveri sono più poveri e i ricchi sono meno poveri.
                                * Voce 1: La forbice si allarga! Perché se io ho voglia di imparare, allora mi metto là, come hai fatto tu, e imparo. Ma se io sono un dipendente di una Pubblica Amministrazione che non sono né incentivato, né premiato, né niente, a me chi me lo fa fare di ammazzarmi al lavoro? La forbice inevitabilmente si allarga.
                                * Voce 3: In questo momento la rapidità è tale, ragazzi, siamo tutti spiazzati, ma anche il mondo della ricerca è rimasto spiazzato. Posso garantire che dentro al nostro istituto, quando è venuto fuori OpenAI, c'è stato un momento di email tipo: «Oddio, cosa facciamo adesso del nostro futuro?», perché sembrava che avessero già fatto tutto. Chi sapeva fare la virgola si sentiva Dio, ora vede che quello che fa lui lo può fare tranquillamente un altro.
                                * Voce 4: Il lavoro che ha fatto Gabriele con la storia... ragazzi, si sono messi loro lì. È arrivato Squad, «Ciao, quella cosa che tu hai fatto, io gliela chiedo e lui mi risponde». Non è così, non è così! La devi pensare prima! Se non riesci a pensarla... Però vi invito davvero a cercare di capire quale genere di domande potremmo fare fra due anni per avere delle risposte, senza che stiamo lì a pensare a quello che non funziona, al sindaco, a quell'altro e all'utente giovane...
                                * Voce 5: Se ci pensate, è proprio il pane per la PA, perché la Pubblica Amministrazione, teoricamente, è regolata da norme che hanno una logica. Uno strumento così, che continuerà ad imparare chi siamo noi, come vogliamo sentirci dire le cose, quali tipi di problemi accogliamo e quali altri invece per noi sono dei vincoli che assolutamente devono essere superati... loro lo sapranno. Non c'è discorsività lì, è logica, e quindi diventerà sempre più semplice rispondere, per come noi vogliamo, pur ponendo malamente le domande.
                                * Voce 6: Sarà una giustificazione per la PA che non deve migliorare, perché tanto c'è il modello che migliora.
                                * Voce 7: Siamo ancora lontani, e questo ci tengo a dirlo, dal fatto che capiscano. Per ristrutturare il dato con una semantica, eccetera, tu puoi fare certe cose...
                                * Voce 8: No, secondo me no. Cioè, perché la gente pensa che arriveranno a capire come capiamo noi? Ma siete lontani da questa cosa, lontanissimo. Il problema è se arriveranno a capire come *non* capiamo noi. È anche peggio, perché magari vanno avanti e ci portano da un'altra parte. E già ce l'abbiamo il problema adesso.
                                * Voce 9: Copiare lo stile... la grande dicotomia nella storia dell'intelligenza artificiale è quella che diceva prima Giorgia, no? Simbolica o non simbolica. Quella roba che stiamo usando oggi non è simbolica, quindi la conoscenza non è formalizzata. È una costellazione di minchiate nel cielo che più o meno, probabilisticamente, funzionano. Io credo moltissimo nell'approccio formalizzato, perché se la legge dice che tu a 18 anni sei maggiorenne e io lo dico all'AI, è quella cosa lì, non è &quot;probabilmente&quot;.
                                * Voce 10: Parliamo, ad esempio, di stile, di creare lo stile di un determinato artista. Ha creato quello stile magari perché prima non esisteva, è andato al di là di ciò che è concepito come normale. E siccome comunque gli LLM vanno a pescare dalla cima della distribuzione normale, secondo me si rischia un po' di limitare la creatività. Si rischia che i nuovi artisti si possano appoggiare su questo comfort di dire: «Faccio un quadro uguale a questo stile» o «Scrivo in quest'altro stile», invece di pensare che le cose possano essere fatte in un modo totalmente diverso. Un po' quello che di solito fa l'arte: andare al di là di ciò che è concepito come normale. Anche a livello di codice, quando si utilizza tantissimo il co-pilota... la cosa che funziona magari non è la più bella, non è la più efficiente, però funziona. E magari si rischia che grandi società, anche le PA, si affidino a questa cosa che funziona e che magari apra tantissime falle che potrebbero essere gestite in maniera più efficiente, più bella. Questo è un grosso tema.
                                * Voce 11: Io ho studiato informatica: &quot;basta che funzioni&quot; è proprio una roba mia! [Applauso]">Leggi la trascrizione</button>
                            </div>
                        </div>
                    </div>
                </div>
            </section>
            
            <!-- Giorno 2 -->
            <section class="mt-16">
                 <h2 class="text-3xl font-bold text-sky-300 border-b-2 border-sky-500/30 pb-3 mb-8">Interventi di Sabato 14 giugno</h2>
                 <div class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-8">
                     <!-- Card Intervento 6 -->
                    <div class="bg-slate-800/50 backdrop-blur-sm border border-blue-800/50 rounded-xl p-6 flex flex-col justify-between shadow-lg hover:shadow-sky-500/20 transition-all duration-300">
                        <div class="flex-grow">
                            <h3 class="text-xl font-semibold text-white mb-2">Allenare il Futuro: Come i Data Hackathon Sviluppano Talenti e Idee Vincenti</h3>
                            <p class="text-slate-400 mb-2">Antonella Longo (UniSalento)</p>
                            <p class="text-slate-300 text-sm mb-6">Il DataLab di UniSalento.</p>
                        </div>
                        <div class="mt-auto pt-4 border-t border-slate-700/60">
                           <div class="flex flex-wrap items-center justify-between gap-3">
                                <div class="flex items-center gap-3">
                                     <a href="resources/slides/longo.pdf" class="text-center bg-slate-700 hover:bg-slate-600 text-white font-semibold py-2 px-3 rounded-lg transition-colors flex items-center justify-center gap-2">
                                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M3 17a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zm3.293-7.707a1 1 0 011.414 0L9 10.586V3a1 1 0 112 0v7.586l1.293-1.293a1 1 0 111.414 1.414l-3 3a1 1 0 01-1.414 0l-3-3a1 1 0 010-1.414z" clip-rule="evenodd" /></svg>
                                        <span>Slide</span>
                                    </a>
                                </div>
                            </div>
                        </div>
                    </div>

                    <!-- Card Intervento 7 -->
                     <div class="bg-slate-800/50 backdrop-blur-sm border border-blue-800/50 rounded-xl p-6 flex flex-col justify-between shadow-lg hover:shadow-sky-500/20 transition-all duration-300">
                        <div class="flex-grow">
                            <h3 class="text-xl font-semibold text-white mb-2">LidoLink e il Futuro del Turismo</h3>
                            <p class="text-slate-400 mb-2">Showbik Showmma</p>
                            <p class="text-slate-300 text-sm mb-6">L'AI che Trasforma l’Esperienza di Viaggio in Tempo Reale</p>
                        </div>
                        <div class="mt-auto pt-4 border-t border-slate-700/60">
                           <div class="flex flex-wrap items-center justify-between gap-3">
                                <div class="flex items-center gap-3">
                                     <a href="resources/slides/showmma.pdf" class="text-center bg-slate-700 hover:bg-slate-600 text-white font-semibold py-2 px-3 rounded-lg transition-colors flex items-center justify-center gap-2">
                                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M3 17a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zm3.293-7.707a1 1 0 011.414 0L9 10.586V3a1 1 0 112 0v7.586l1.293-1.293a1 1 0 111.414 1.414l-3 3a1 1 0 01-1.414 0l-3-3a1 1 0 010-1.414z" clip-rule="evenodd" /></svg>
                                        <span>Slide</span>
                                    </a>
                                </div>
                            </div>
                        </div>
                    </div>
                 </div>
            </section>
            
            <!-- Risultati Datathon -->
            <section class="mt-16">
                <h2 class="text-3xl font-bold text-sky-300 border-b-2 border-sky-500/30 pb-3 mb-8">Risultati del Datathon</h2>
                <div class="grid grid-cols-1 md:grid-cols-2 gap-8">

                    <!-- Card Track 1 -->
                    <div class="bg-slate-800/50 backdrop-blur-sm border border-blue-800/50 rounded-xl p-6 flex flex-col justify-between shadow-lg hover:shadow-sky-500/20 transition-all duration-300">
                        <div class="flex-grow">
                            <h3 class="text-xl font-semibold text-white mb-2">Track 1: Qualità dei Dati</h3>
                            <p class="text-slate-300 text-sm mb-6">Lorem ipsum.</p>
                        </div>
                        <div class="mt-auto pt-4 border-t border-slate-700/60">
                            <div class="flex flex-wrap items-center justify-start gap-3">
                                <a href="#" class="text-center bg-sky-600 hover:bg-sky-700 text-white font-semibold py-2 px-4 rounded-lg transition-colors flex items-center justify-center gap-2">
                                    <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor">
                                      <path d="M7 4a3 3 0 016 0v6a3 3 0 11-6 0V4z" />
                                      <path d="M5.5 8.5a.5.5 0 01.5.5v1a4 4 0 004 4h0a4 4 0 004-4v-1a.5.5 0 011 0v1a5 5 0 01-4.5 4.975V17h3a.5.5 0 010 1h-7a.5.5 0 010-1h3v-1.525A5 5 0 014.5 9.5v-1a.5.5 0 01.5-.5z" />
                                    </svg>
                                    <span>Ascolta</span>
                                </a>
                                <button class="modal-trigger text-center bg-slate-700 hover:bg-slate-600 text-white font-semibold py-2 px-4 rounded-lg transition-colors flex items-center justify-center gap-2" data-title="Riepilogo: Track Qualità dei Dati" data-content="Lorem ipsum">
                                    <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor">
                                      <path fill-rule="evenodd" d="M4 4a2 2 0 012-2h8a2 2 0 012 2v12a1 1 0 110 2h-3a1 1 0 01-1-1v-2a1 1 0 00-1-1H9a1 1 0 00-1 1v2a1 1 0 01-1 1H4a1 1 0 110-2V4zm2 1a1 1 0 011-1h1a1 1 0 110 2H7a1 1 0 01-1-1zm0 4a1 1 0 011-1h1a1 1 0 110 2H7a1 1 0 01-1-1zm0 4a1 1 0 011-1h1a1 1 0 110 2H7a1 1 0 01-1-1z" clip-rule="evenodd" />
                                    </svg>
                                    <span>Riepilogo</span>
                                </button>
                            </div>
                        </div>
                    </div>

                    <!-- Card Track 2 -->
                    <div class="bg-slate-800/50 backdrop-blur-sm border border-blue-800/50 rounded-xl p-6 flex flex-col justify-between shadow-lg hover:shadow-sky-500/20 transition-all duration-300">
                        <div class="flex-grow">
                            <h3 class="text-xl font-semibold text-white mb-2">Track 2: Arricchimento Dati Acqua con WHOW</h3>
                            <p class="text-slate-300 text-sm mb-6">Questo gruppo ha esplorato lorem ipsum</p>
                        </div>
                         <div class="mt-auto pt-4 border-t border-slate-700/60">
                            <div class="flex flex-wrap items-center justify-start gap-3">
                                <a href="#" class="text-center bg-sky-600 hover:bg-sky-700 text-white font-semibold py-2 px-4 rounded-lg transition-colors flex items-center justify-center gap-2">
                                    <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor">
                                      <path d="M7 4a3 3 0 016 0v6a3 3 0 11-6 0V4z" />
                                      <path d="M5.5 8.5a.5.5 0 01.5.5v1a4 4 0 004 4h0a4 4 0 004-4v-1a.5.5 0 011 0v1a5 5 0 01-4.5 4.975V17h3a.5.5 0 010 1h-7a.5.5 0 010-1h3v-1.525A5 5 0 014.5 9.5v-1a.5.5 0 01.5-.5z" />
                                    </svg>
                                    <span>Ascolta</span>
                                </a>
                                <button class="modal-trigger text-center bg-slate-700 hover:bg-slate-600 text-white font-semibold py-2 px-4 rounded-lg transition-colors flex items-center justify-center gap-2" data-title="Riepilogo: Track Arricchimento Dati Acqua" data-content="Lorem ipsum">
                                    <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor">
                                      <path fill-rule="evenodd" d="M4 4a2 2 0 012-2h8a2 2 0 012 2v12a1 1 0 110 2h-3a1 1 0 01-1-1v-2a1 1 0 00-1-1H9a1 1 0 00-1 1v2a1 1 0 01-1 1H4a1 1 0 110-2V4zm2 1a1 1 0 011-1h1a1 1 0 110 2H7a1 1 0 01-1-1zm0 4a1 1 0 011-1h1a1 1 0 110 2H7a1 1 0 01-1-1zm0 4a1 1 0 011-1h1a1 1 0 110 2H7a1 1 0 01-1-1z" clip-rule="evenodd" />
                                    </svg>
                                    <span>Riepilogo</span>
                                </button>
                            </div>
                        </div>
                    </div>
                </div>
            </section>
        </main>

        <!-- Footer -->
        <footer class="text-center mt-20 pt-10 border-t border-slate-700">
            <p class="text-slate-400">
                Evento organizzato nell'ambito del progetto europeo <strong class="font-semibold text-slate-300">SMERALD</strong>.
            </p>
            <p class="text-sm text-slate-500 mt-2">
                2023-1-IT01-KA220-VET-000151990
            </p>
        </footer>

    </div>

    <!-- Modal -->
    <div id="info-modal" class="fixed inset-0 bg-black bg-opacity-70 backdrop-blur-sm flex items-center justify-center p-4 hidden z-50">
        <div class="bg-slate-800 border border-blue-800 rounded-xl shadow-2xl w-full max-w-2xl max-h-[80vh] flex flex-col">
            <div class="flex justify-between items-center p-4 border-b border-slate-700">
                <h3 id="modal-title" class="text-xl font-semibold text-sky-300"></h3>
                <button id="modal-close" class="text-slate-400 hover:text-white transition-colors">
                    <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2">
                        <path stroke-linecap="round" stroke-linejoin="round" d="M6 18L18 6M6 6l12 12" />
                    </svg>
                </button>
            </div>
            <div id="modal-content" class="p-6 overflow-y-auto modal-content text-slate-300">
                <!-- Contenuto dinamico qui -->
            </div>
        </div>
    </div>
    
    <script>
        document.addEventListener('DOMContentLoaded', () => {
            const modal = document.getElementById('info-modal');
            const modalTitle = document.getElementById('modal-title');
            const modalContent = document.getElementById('modal-content');
            const modalClose = document.getElementById('modal-close');
            
            // Gestione Modal
            document.querySelectorAll('.modal-trigger').forEach(trigger => {
                trigger.addEventListener('click', () => {
                    const title = trigger.getAttribute('data-title');
                    const content = trigger.getAttribute('data-content');
                    
                    modalTitle.textContent = title;
                    modalContent.innerHTML = content.replace(/\n/g, '<br>');
                    modal.classList.remove('hidden');
                });
            });

            const closeModal = () => modal.classList.add('hidden');
            modalClose.addEventListener('click', closeModal);
            modal.addEventListener('click', (event) => {
                if (event.target === modal) closeModal();
            });

            // Gestione Accordion
            document.querySelectorAll('.expand-trigger').forEach(button => {
                button.addEventListener('click', () => {
                    const content = button.parentElement.nextElementSibling;
                    const chevron = button.querySelector('.chevron');
                    
                    if (content.style.maxHeight) {
                        content.style.maxHeight = null;
                        chevron.classList.remove('rotate-180');
                    } else {
                        // Aggiungiamo un piccolo buffer per sicurezza
                        content.style.maxHeight = (content.scrollHeight + 10) + "px";
                        chevron.classList.add('rotate-180');
                    } 
                });
            });

            // Chiudi modal con 'Esc'
            document.addEventListener('keydown', (event) => {
                if (event.key === 'Escape' && !modal.classList.contains('hidden')) {
                     closeModal();
                }
            });
        });
    </script>

</body>
</html>
